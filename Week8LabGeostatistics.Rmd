---
title: "Week 8 Lab:  Geostatistics"
author: "Lianne Sheppard for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%d %B, %Y')`"
output: 
    html_document:
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

knitr::opts_chunk$set(echo = TRUE)

```

```{r clear.workspace, eval=FALSE, echo=FALSE}
#-----clear workspace-----
# Clear the environment without clearing knitr
#
# This chunk is useful for code development because it simulates the knitr
# environment. Run it as a code chunk when testing. When knitr is run, it uses a
# fresh, clean environment, so we set eval=FALSE to disable this chunk when
# rendering.

# Clear workspace of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE)
        )
}

```

```{r load.libraries.pacman, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries pacman-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.  Some reasons for packages:
# knitr:  kable()
# ggplot2: part of tidyverse
# readr: file reading
# dplyr: data wrangling
# Hmisc:  describe
# gstat:  for kriging
# maps: for maps
# sf:  will eventually replace sp: spatial data methods, the modern "simpler"
#   version (see
#   https://www.nickbearman.me.uk/2019/04/spatial-r-moving-from-sp-to-sf/ for
#   details)
# funModeling:  for EDA
# scales: muted and other color scale functions
# akima: gridded bivariate interpolation for irregular data
# sp: spatial data methods -- will eventually be superseded by `sf`
# rgdal:  projections, coordinate systems
# downloader: download files over HTTP and HTTPS
pacman::p_load(tidyverse, knitr, dplyr, maps, ggmap, stars,
               funModeling, Hmisc, scales, akima, sp, rgdal, downloader, sf)
# Install development version of gstat
# As of February 2021, an important bugfix has not yet made it onto Cran
# Next time this course is taught, should be able to remove this line and
# place gstat in the p_load list above
pacman::p_load_gh('r-spatial/gstat')
if (packageVersion('gstat') < '2.0.7'){
  message('Requires gstat version >= 2.0.7')
  message('Try `remove.packages('gstat')`, then quit Rstudio and run this chunk again')
}

# Note: this lab will only knit if the latest version of ggmap is installed.  
#   The following line does this, but needs to be run at the console before
#   knitting.  (Note: Installation of ggmap takes a LONG time.)
#pacman::p_load(ggmap, install = TRUE, update = TRUE)

```

<!-- TODO:  Get the datasets to Brian's directory
The reason for the changes on 3/5/19 had to do with needing lambert coordinates and maybe using a smaller grid for the predictions -->
```{r read.data.from.a.directory, eval=TRUE, echo=FALSE, include=FALSE}
#-----read data from a directory-----
# Download the data file from a local directory

datapath <- "Datasets"
dir.create(datapath, showWarnings=FALSE, recursive = TRUE)

snapshot.file <- "snapshot_3_5_19.csv"
grid.file <- "la_grid_3_5_19.csv"
snapshot.path <- file.path(datapath, snapshot.file)
grid.path <- file.path(datapath, grid.file)

# note:  updated shapshot data not on website yet
if (file.exists(snapshot.path)) {
    snapshot <- read_csv(file = snapshot.path)
} else warning(paste("Can't find", snapshot.file, "!"))

# note:  la_grid not on website yet
if (file.exists(grid.path)) {
    la_grid <- read_csv(file = grid.path)
} else warning(paste("Can't find", grid.file, "!"))

```

# Introduction and Purpose

The purpose of this lab is to learn about geostatistical models and further
solidify your understanding of regression for prediction.  We will use the same
MESA Air snapshot data described in Mercer et al 2011 that we used earlier in
the quarter.

TODO:  UPdate:  **Important Winter 2021 note**:  This lab is a work in progress
and it has some unfinished pieces.  I believe that all included analyses have
been sufficiently tested and debugged, but it is also possible some errors
remain.  In addition, there are a number of ideas that need further
investigation.  The intent of the current version of this lab is to help you
learn to use geographic data in R as productively as possible while recognizing
there is much more to learn and develop.

# Getting Started

## Introductory comments 

### Resources for Spatial data in R

The use of spatial data in R is rapidly evolving.  I have found several useful
overview resources:

* A good and recently updated book is  [Geocomputation in
R](https://geocompr.robinlovelace.net/index.html) by Robin Lovelace.  Its
[introductory chapter](https://geocompr.robinlovelace.net/intro.html) gives a
great overview of the "what" and "why" of geocomputation.  It also has a very
nice overview of the history of spatial data in R, which Lovelace calls [The
history of
R-spatial](https://geocompr.robinlovelace.net/intro.html#the-history-of-r-spatial).
In particular, it puts into historical context the tools we are using in this
lab with the current state of the art.  Clearly, more could be done to
demonstrate the current state of the art for R-spatial data.

* Roger Bivand has done an incredible amount of work with spatial data in R. His
book, [Spatial Data Analysis in R](https://asdar-book.org/) was published in
2013.  (The link is actually to the scripts and datasets to reproduce the
examples in his book.)  It is available for download from the UW library and
there is a copy on the course website. In particular, I recommend Chapter 1 for
an overview of ideas, Chapter 4 for the quickest start to using spatial data
(though the rest of Part I will also be helpful), and Chapter 8 for methods for
kriging.

* Pebesma & Bivand have a new book [Spatial Data
Science](https://keen-swartz-3146c4.netlify.app/) that explains the concepts
underlying spatial data and links it to many modern packages of theirs (e.g.
`sf`, `lwgeom`, and `stars`) and accompanies this with `tidyverse`.

* This RPub describes [Spatial Data
Objects](https://rpubs.com/NateByers/spatialobjects) including `Spatial`,
`SpatialPoints`, and `SpatialPointsDataFrame`.

See also the introduction to the snapshot data section below for a discussion on
distance calculations.

### R packages for geostatistics and spatial data

Packages for kriging include `geoR`, `automap`, and `gstat`.  The package `geoR`
has been around for some time and the version of this lab develped 2 years ago
used this package.  The package `gstat` is newer and uses the `sp` class which
has become a standard for geographic data (though it appears that `sf` may
eventually replace it).  `automap` calls `gstat`.  `geoR` and `gstat` appear to
give identical results, at least for a simple dataset example.  To verify this,
type `demo(gstat::comp_geoR)` in the console after loading both `gstat` and
`geoR`.  Because of its apparent current use, we discuss `gstat` in this lab.
There are good examples online and in books (e.g., Bivand's) that show how to do
kriging with `gstat`.

(Aside:  There is an interesting [stackoverflow
discussion](https://stackoverflow.com/questions/21970992/compare-variogram-and-variog-function)
of comparing empirical variograms in `geoR` vs. `sp` and `gstat` that addresses
coordinate transformations and binning.  It is worth a look if you want to
understand the details better.)

**Note on `rgdal`**:  If you are running your lab locally on a Mac, this package
doesn't load properly on newer Macs running Catalina.  You need to install GDAL
from the terminal.  See this
[link](https://medium.com/@egiron/how-to-install-gdal-and-qgis-on-macos-catalina-ca690dca4f91)
for how to do this.

### Some comments on universal kriging and prediction  

In kriging, you can't predict on the same locations that you used to estimate
the parameters.  As discussed on
[stackoverflow](https://stackoverflow.com/questions/45768516/why-does-the-kriging-give-the-same-values-as-the-observed),
"this is a well-known property of kriging; it comes from the fact that the model
underlying kriging assumes that a value is perfectly correlated with itself."
Thus predicting a known value always returns that value, with zero prediction
error.  This is also the cause of errors with duplicate observations at the same
location. Think of this property of kriging as an enforced need to do
cross-validation to evaluate your predictions.  Apparently with the `gstat`
package you can do smoothing instead of kriging by specifying an "Err"
variogram instead of a "Nug" nugget effect.

### Brief discussion of variograms

A semivariance is defined as one half of the average of all the squared
differences of all points that are a certain distance $h$ apart.  A variogram is
a plot of these squared differences as a function of distance, presented as
either all the points (a "cloud"), or as an average (typically called "binned").
Empirical variograms are useful for helping a data analyst see the spatial
structure in the data.  In many situations with spatial structure, the average
semivariance increases as distance increases until it eventually levels off.  In
geostatistics we use a model to approximate the structure we see in an empirical
variogram.  There are different variogram models depending upon the assumed
covariance model.  Examples of common assumed models are exponential, spherical,
Gaussian, and Matern.  This [link](http://www.kgs.ku.edu/Tis/surf3/s3krig2.html)
gives a brief summary of semivariance and variograms (which it calls
semivariograms; see the following link for discussion of terminology and the
confusing overlapping terms in the literature).

There is confusion about terminology and often variogram and semivariogram are
used interchangeably.  This paper, [Variogram or semivariogram? Understanding
the variances in a
variogram](https://link.springer.com/article/10.1007%2Fs11119-008-9056-2), gives
a nice overview of the terminology and notation.  It also links well-understood
estimates of variance with semivariance estimates, making the point that a
variance can be expressed either in the traditional way as the squared deviation
from the mean, or as 1/2 the average of squared differences between points,
averaged over all possible pairs of points.  This helps you see that the formula
for a variogram is a re-expression of a standard variance formula, though now
looking at each term that goes into the sum as a function of distance.


## Applications using `gstat`: Practice with the built-in `meuse` dataset

The purpose of this section is to show some `gstat` tools using the built-in
`meuse` dataset.  The dataset is part of the package `sp` and documentation of
it can be found
[here](http://www.dpi.inpe.br/gilberto/tutorials/software/R-contrib/sp/html/meuse.html).

First we will go through the `meuse` dataset, a dataset of soil contamination
from The Netherlands.  Its location coordinates are `x` and `y`, and its
exposure measurements are concentrations of various metals in topsoil.  We will
focus on `zinc` in this lab.  There are also some potential covariates we could
consider in a universal kriging model such as elevation and distance from the
Meuse river.


### Summary & learn about the data

The following section follows the example [here](https://rpubs.com/liem/63374) as
well as a more updated version that uses `tidyverse` and `ggplot` commands called
[Meuse
Tutorial](https://rstudio-pubs-static.s3.amazonaws.com/134781_28af3676f8b943749ebfa536b3897cac.html).

```{r basics with meuse dataset - summary + EDA}
# ------------ basics with meuse dataset - summary -----------
# load the data
data(meuse)

# learn about this dataset
# currently it is just a raw data.frame
class(meuse)
head(meuse)

# glimpse and str are both useful to learn the structure.  I like glimpse from the `dplyr` package, particularly once this becomes converted to a spatial dataset
str(meuse)
glimpse(meuse)

# summary of the data
summary(meuse) 

# So far, R does not know that this is spatial data.
# Now turn meuse into a spatial dataset
# We specify that the XY coordinates are in columns `x` and `y`
meuse <- st_as_sf(meuse, coords=c('x','y'))
# It is now both an sf object and a data.frame
class(meuse)

# Although the xy columns are now recognized as the coordinates,
# they still have no explicit spatial meaning without a projection
st_crs(meuse)
# Look up the projection of the meuse coordinates here.
# This is dataset-specific
# https://rdrr.io/cran/sp/man/meuse.html
# The proj4 string is a standardized way to describe a projection
# Get the proj4string here:
# https://epsg.io/28992
# Set the projection, in this case a projection commonly used in the Netherlands
st_crs(meuse) <- '+proj=sterea +lat_0=52.15616055555555 +lon_0=5.38763888888889 +k=0.9999079 +x_0=155000 +y_0=463000 +ellps=bessel +towgs84=565.417,50.3319,465.552,-0.398957,0.343988,-1.8774,4.0725 +units=m +no_defs'
# Can also just use the EPSG string
#st_crs(meuse) <- 28992


```

#### Comments about SpatialPointsDataFrame objects

By specifying the `coordinates()` above, meuse becomes a SpatialPointsDataFrame
(SPDF).  This class is a S4 object where key data and attributes are stored in
"slots".  For a SPDF object there are 5 slots:  data, coords.nrs, coords, bbox,
and proj4string.  All the data are stored in the `data` slot.  The coordinates
are stored in the `coordinates` slot.  The location of the coordinates in the
dataframe is stored in `coords.nrs`.  The `bbox` is the bounding box, or the
spatial exent of the data.  It corresponds to the "corners" of a rectangular
map.  Finally, `proj4string` contains the projection for the coordinates.
Projections are essential to know in order to place maps in space correctly.

The next chunk shows how to access values in slots.  

```{r extract meuse data}
# ---- extract meuse data ----

# Confirm the coordinates are properly projected now:
st_crs(meuse)

# View a summary description of our projected data frame
meuse

# bounding box
st_bbox(meuse)

# projection
st_crs(meuse)

# The original data.frame columns (except for X and Y, which are now `geometry`),
# can still be accessed in the usual way
head(meuse)
meuse$zinc
colnames(meuse)

# To extract original XY coordinates
st_coordinates(meuse) %>% head()


```


#### Some meuse plots

```{r meuse plots}
# ---- meuse plots ----

# Read in the Meuse River polygon
data(meuse.riv)
head(meuse.riv,3)
tail(meuse.riv,3)
# Note that row 1 is the same as the last row (this closes the polygon)
str(meuse.riv)
# The meuse.riv object is still just a matrix of raw XY values.
# The following code specifies the columns containing the coordinates,
# specifies the projection of the coordinates as the same as meuse projection,
# and explicitly combines the points into a polygon
meuse.riv <- st_as_sf(data.frame(meuse.riv),coords=1:2,crs=st_crs(meuse)) %>%
  st_combine() %>%
  st_cast('POLYGON')

# ggplot2 readily accepts sf objects for plotting via geom_sf()
# Let's make a map
ggplot(data=meuse) + geom_sf() + geom_sf(data=meuse.riv)

# The river is too long compared to the study area
# For prettier plotting, crop the river polygon to within 500m buffer of data
# This works because our projection is in meter units
# This code buffers the points of the measurements by 500 m,
# obtains the smallest possible rectanglar bounding box around those buffers,
# and crops the river polygon to that rectangle
meuse.riv <- st_crop(meuse.riv,st_bbox(st_buffer(meuse,500)))

# Much better!
ggplot(data=meuse) + geom_sf() + geom_sf(data=meuse.riv)

# Color the zinc data based on the native scale
# Plotting the river before the points ensures points are on top of river
# Scale_color_gradients with rainbow(rev=TRUE) creates a heatmap

ggplot(data=meuse) +
  geom_sf(data=meuse.riv) +
  geom_sf(data=meuse, aes(color=zinc)) +
  scale_color_gradientn(colors=rainbow(4,rev=TRUE)) +
  ggtitle("Zinc Concentration (ppm)") +
  theme_bw()

# Color the zinc data based on the log scale

ggplot(data=meuse) +
  geom_sf(data=meuse.riv) +
  geom_sf(data=meuse, aes(color=log(zinc))) +
  scale_color_gradientn(colors=rainbow(4,rev=TRUE)) +
  ggtitle("Zinc Concentration in log(ppm)") +
  theme_bw()

# Make a bubble plot
# The alpha value adds transparency to plotting shapes

ggplot(data=meuse) +
  geom_sf(data=meuse.riv) +
  geom_sf(data=meuse, aes(size=zinc), color='blue', alpha=1/2) +
  ggtitle("Zinc Concentration (ppm)") +
  theme_bw()

```

Note that geom_sf plots in lat-long units by default, even though our data is stored in a different projection. (As we will see later, it is trivial to convert our points to a different projection once our points have a specific projection in sf). We can check that map projections are working properly and explore the study area on [Google Maps](https://www.google.com/maps?q=50.97,5.74)

### Estimating variograms

#### Empirical variograms

We use variograms to get an understanding on how our variable of interest varies
over space.  One of the variables in the `meuse` dataset is the concentration of
zinc.  We will focus on the log-transformed value of zinc.

The following code gives empirical variograms plotted using two different
options:  a *variogram cloud* with all squared distances (`cloud = TRUE`), and
the default *binned variogram*.  The third plot is the binned variogram again;
this one shows the number of points that make up each bin.  Note that while the
`meuse` dataset has only 155 observations, the variogram is binning over all
possible point pairs: 155 * 154 / 2 = 11,935.  However, the variogram cloud
dataset only has 6833 data points (and the sum of `np` the number of points in
each bin is also 6833), so there must be some truncation we aren't aware of.

TODO:  why fewer than 11935 points?  This is probably because the default
distance is 1/3 of the maximum distance.  We can change the default distance
using the `cutoff =` option.  (See example with the snapshot data.)

```{r meuse empirical variogram, warning=FALSE}
# ---- meuse empirical variogram ----

# plot variogram cloud
plot(variogram(log(zinc)~1, meuse, cloud=TRUE))

# plot binned variogram (the default)
plot(variogram(log(zinc)~1, meuse))

# you can also show the number of points in each bin
plot(variogram(log(zinc)~1, meuse), pl=TRUE)

# save the variogram cloud 
vgm.meuse <- variogram(log(zinc)~1, meuse, cloud=TRUE)

# Now plot the cloud and overlay a smooth curve
# note that the full scatter is misleading relative to the smooth curve
# we could adjust the density to more clearly see where the data are
# according to the gs_intro_20Mar2019.pdf, the distance is Euclidean distance;
#         gamma is the semi-variance estimate
# TOOD:  refine plot w/ labels, adjusted density, using ggplot
plot(vgm.meuse$dist, vgm.meuse$gamma)
lines(smooth.spline(vgm.meuse$dist, vgm.meuse$gamma, df = 6), col="red",lwd = 4)


#vgm.meuse <- variogram(log(zinc)~1, meuse)
#sum(vgm.meuse$np)

```


#### Modeled variogram

One can superimpose various modeled variograms onto empirical variograms. We
need a modeled variogram in order to parameterize the structured error in our
kriging model.  This is important because we need to have some idea of
appropriate variogram parameters because we need to supply the kriging
estimation fuctions with initial values of these parameters, specifically the
partial sill ($\sigma^2$) and range ($\phi$).  The following chunk offers 3
possible variogram model options and the best-fitting one was selected.


```{r meuse modeled variogram}
# ---- meuse modeled variogram -----

# first estimate the variogram object and assign it a name
v <- variogram(log(zinc)~1, meuse)

# Then fit a variogram model, offering to the function several different model
# options (exponential, spherical, and Matern):
v.fit <- fit.variogram(v, vgm(c("Exp", "Sph", "Mat")))

# Find out which variogram model was selected
# Observe that it was the spherical with a range of 897 m, a partial sill of
# 0.59, and a nugget of 0.05.
v.fit

# Plot the empirical variogram with the overlaid fit
plot(v, v.fit)


```

### Kriging

Credit:  The following examples follow the code [here](https://github.com/r-spatial/gstat/blob/master/demo/krige.R).

#### Ordinary Kriging (OK)

We use kriging to get predictions at *new* locations (not used in the model
fitting).  Use the function `krige` to accomplish this.  Give it locations where
it should predict in the `newdata = ` option.  You also need to pass it the
results of a fitted variogram model in the `model = ` option.  Note that in this
example we are estimating a common mean using ordinary kriging.

```{r meuse ordinary kriging}
# ---- meuse ordinary kriging ----

# read in the meuse grid to use for predictions
data(meuse.grid)
# convert to sf points
meuse.grid <- st_as_sf(meuse.grid,coords=c('x','y'),crs=st_crs(meuse))
#gridded(meuse.grid) = ~x+y

# ordinary kriging of log(zinc)
# first two arguments are formula and data; krige doesn't like explicit
# reference to them (??)
lzn.kr <- krige(log(zinc)~1, meuse, newdata = meuse.grid, model = v.fit)

# plot kriging predictions and SEs
pl1 <- plot(lzn.kr[c('var1.pred','var1.var')], main = "OK prediction of log-zinc")
#pl1

lzn.kr$se = sqrt(lzn.kr$var1.var)
pl2 <- plot(lzn.kr["se"], main = "OK prediction error")
#pl2

```

#### Universal kriging (UK)

To do universal kriging, we also need covariates for the fixed part of the
model.  As discussed in Mercer et al, ArcGIS doesn't (or didn't at the time that
paper was written) allow an arbitrary set of covariates to be included in UK.
ArcGIS only allows (or allowed) the mean function to be a function of latitude
and longitude, which is far too limiting in many applications.  The next chunk
demonstrates universal kriging with the `meuse` data.   We demonstrate UK using
`gstat` with the snapshot data in the next section.


```{r meuse universal kriging, warning=FALSE}
# ---- meuse universal kriging ----

# First we need to fit a new variogram model with the square root of distance
# from the Meuse River as a covariate.  And estimate its variogram model
v.uk <- variogram(log(zinc)~sqrt(dist), meuse)
m.uk <- fit.variogram(v.uk, vgm("Exp","Sph","Mat"))

# learn about the best-fitting variogram model and plot it
m.uk
plot(v.uk, model = m.uk)

# fit the universal kriging model, predicting on the meuse.grid
lzn.kr <- krige(log(zinc)~sqrt(dist), meuse, meuse.grid, model = m.uk)

# plot the UK predictions and SEs
pl3 <- plot(lzn.kr[c('var1.pred','var1.var')], main = "UK prediction of log-zinc")
lzn.kr$se = sqrt(lzn.kr$var1.var)
pl4 <- plot(lzn.kr["se"], main = "UK prediction error")
# 
# # Now plot all 4 kriging results on a single plot, which allows easy comparison
# print(pl1, split = c(1,1,2,2), more = T)
# print(pl2, split = c(1,2,2,2), more = T)
# print(pl3, split = c(2,1,2,2), more = T)
# print(pl4, split = c(2,2,2,2))

```

### Cross-validating the kriging model

#### Cross-validation using the built-in CV function `krige.cv`

`gstat` uses the function `krige.cv` to do kriging with cross-validation. One
passes it the number of folds as a scalar and then it randomly divides the data
into that number of folds.  The default is leave one out (LOO) which is a scalar
of the length of the data.  One can also pass to nfolds a vector of the length
the data with values to indicate the groups each observation belongs in (e.g.
the cluster variable in the snapshot dataset).

TODO test/expand:  need to identify how the code works with the various options
and explore the output.  Update to align with the didactic goals of the lab
which is to show geostatistics tools and to leverage our understanding of
cross-validation for prediction performance.

```{r meuse cross-validation, warning=FALSE}
# ---- meuse cross-validation ----

# Define function to create a bubble plot for kriging residuals
krige.cv.bubble <- function(cv.out, plot_title){
  ggplot(data=cv.out) +
  geom_sf(aes(size=abs(residual), color=factor(residual>0)), alpha=1/2) +
  scale_color_discrete(name='residual > 0', direction=-1) +
  scale_size_continuous(name='|residual|') +
  ggtitle(plot_title) +
  theme_bw()
}

# Define function for calculating the MSE, RMSE, MSE-based R2 from krige.cv output
krige.cv.stats <- function(krige.cv.output){
  d <- krige.cv.output
  # mean of observations
  mean_observed <- mean(d$observed)
  # MSE of predictions
  MSE_pred <- mean((d$observed - d$var1.pred)^2)
  # MSE of observations (for R2 denominator)
  MSE_obs <- mean((d$observed - mean_observed)^2)
  # print the results not rounded
  cat(paste("RMSE:  ", sqrt(MSE_pred)),'\n')
  cat(paste("MSE-based R2:  ", max(1 - MSE_pred/MSE_obs, 0)),'\n')
}


# ordinary kriging, 5-fold cross-validation 
# nmax defaults to all observations used for kriging; this sets it at 40
# TODO:  investigate the difference
meuse.CV5 <- krige.cv(log(zinc)~1, meuse, model = v.fit, nmax = 40, nfold=5)
# stats
krige.cv.stats(meuse.CV5)
# plot the residuals
krige.cv.bubble(meuse.CV5,"log(zinc): 5-fold CV residuals")


# ordinary kriging, LOOCV
meuse.CVLOO <- krige.cv(log(zinc)~1, meuse, model = v.fit, nmax = 40)
# stats
krige.cv.stats(meuse.CVLOO)
# plot the residuals
krige.cv.bubble(meuse.CVLOO, "log(zinc): LOO CV residuals")


# universal kriging, 5-fold cross-validation 
meuse.CV5uk <-
  krige.cv( log(zinc) ~ sqrt(dist), meuse,
    model = m.uk, nmax = 40, nfold = 5 )
# stats
krige.cv.stats(meuse.CV5uk)
# plot the residuals
krige.cv.bubble(meuse.CV5uk,"log(zinc) UK on sqrt(dist): 5-fold CV residuals")


# universal kriging, LOOCV
meuse.CVLOOuk <- krige.cv(log(zinc)~sqrt(dist), meuse, 
                        model = m.uk, nmax = 40)
# stats
krige.cv.stats(meuse.CVLOOuk)
# plot the residuals
krige.cv.bubble(meuse.CVLOOuk,"log(zinc) UK on sqrt(dist): LOO CV residuals")

```


#### Kriging using our own cross-validation code

TODO:  The following chunk integrates kriging with our cross-validation function
from lab 4 and shows the same performance statistics.


## Geostatistical analysis using the Snapshot data

### Comments about geographic coordinates and conversions

It is important to know the projection used in order to correctly do distance
calculations.  Using distance calculations based on the Pythagorean theorem
won't give you correct distances on a sphere.  Thus we can't use latitude and
longitude directly.  The Mercer dataset we are using for this lab has three location variables:

* `latitude` and `longitude` which is in decimal degrees
* `lat_m` and `long_m`, which may be in UTM, and which supposedly had
been projected to a flat surface, and
* `lambert_x` and `lambert_y` which is a Lambert projection and is in meters.
It is the USA_Continguous_Lambert_Conformal_Conic.

We are going to use [Lambert
coordinates](https://en.wikipedia.org/wiki/Lambert_conformal_conic_projection)
for this dataset because these are in meters and projected for a flat
surface so distance calculations can be done directly.  To see a documentation
of the projection formulas, see this [New Zealand government
website](https://www.linz.govt.nz/data/geodetic-system/coordinate-conversion/projection-conversions/lambert-conformal-conic-geographic)

There are also some useful packages and functions for working with spatial data
and to get distances.  Some examples are:

* The `sp::SpatialPoints` function can is used to create objects of the spatial
points from lat/long data.

* The `sf` package is the modern replacement for package sp.

* The `rgeos::gWithinDistance` function is used to find if locations are the
same, or within a specified distance.

* The `rgeos::gDistance` function is used to find the Cartesian minimum distance
between two locations. This function can be used to create a distance matrix.

* The `geosphere` package has  a bunch of distance formulas for two points with
latitude and longitude coordinates.  See this [stackoverflow
comment](https://stackoverflow.com/questions/31668163/geographic-geospatial-distance-between-2-lists-of-lat-lon-points-coordinates).

### Snapshot data set-up

First read in the snapshot data as a SpatialPointsDataFrame object.  We are only
using the fall season for this example.  Summarize the data.  Take note of the
range of the data coordinates, the maximum distance, and other dataset features
(e.g. which covariates are included).

```{r read fall snapshot as a SPDF}
# save coordinate systems as variables
# WGS84 latitude-longitude
latlong_proj <- "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs"
# Lambert Conic projection (meters)
lambert_proj <- "+proj=lcc +lat_1=33 +lat_2=45 +lat_0=39 +lon_0=-96 +x_0=0 +y_0=0 +datum=NAD83 +units=m +no_defs"

# focus only on the common model covariates
# Not importing redundant spatial columns like lat_m, long_m, lambert_x, lambert_y,
# Because we can easily reproject the data however we like later
fall <- snapshot %>%
    filter(seasonfac == "2Fall") %>%
    select(ID, latitude, longitude, ln_nox, D2A1, A1_50, A23_400, Pop_5000, D2C, Int_3000, D2Comm, cluster, group_loc, FieldID) %>%
  as.data.frame

# now convert this to an sf object
fall <- st_as_sf(fall, coords=c('longitude','latitude'), crs=latlong_proj)

# get summary information and column names
fall

# see the raw XY (long-lat) coordinates
st_coordinates(fall) %>% head()

# # maximum distance in the dataset; thought I would use this to set the cutoff in
# # the variogram, but it didn't work so I hard-coded it.

# Temporarily convert to lambert projection to get bounding box in meters
#maxdist <- fall %>% st_transform(lambert_proj) %>% st_bbox() %>% dist() %>% max()


```

For later use, we need to convert the grid to sf points:  

```{r convert LA grid to SPDF}
#-----convert LA grid to SPDF-----

class(la_grid)

# la_grid is a tibble; convert to a data frame (needed?)
# (We could just import them as data.frame at the beginning with read.csv())
# Remove redundant lambert columns, we only need one coordinate system
# also drop -Inf values in D2A1 variable
la_grid <- la_grid %>%
  filter(D2A1 != -Inf) %>%
  as.data.frame() %>%
  select(-lambert_x,-lambert_y) %>%
  st_as_sf(coords=c('longitude','latitude'), crs=latlong_proj)

# get info
la_grid

# Not sure we need to tell R that this is gridded and it is failing b/c
# coordinate intervals are not constant. The code works without this:
# # gridded(la_grid) <- TRUE
# examples suggest this shortcut works too:
#gridded(la_grid) <- ~lambert_x+lambert_y


# Remove the water points from la_grid
# (We don't have data from the water, so we do not want to predict there)

# specify url
url <- "https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/physical/ne_10m_land.zip"

# download zip file
# note: if on Windows, the argument `mode = "wb"` (Windows binary) must be added
if (!file.exists(file.path('Datasets','ne_10m_land.zip'))){
  download.file(url = url, 
                destfile = file.path('Datasets','ne_10m_land.zip'))
}

# unzip file
if (!file.exists(file.path('Datasets','ne_10m_land','ne_10m_land.shp'))){
  unzip(file.path('Datasets','ne_10m_land.zip'), 
        exdir = file.path('Datasets','ne_10m_land'))
}

# read shapefile as sf multipolygon
land <- st_read(file.path('Datasets','ne_10m_land','ne_10m_land.shp'))
# crop world land multipolygon to that which overlaps la_grid
# to save time in next step
land <- suppressWarnings(st_crop(land,st_bbox(la_grid)))
# Show map of the cropped land object
ggplot(land) + geom_sf(aes(fill=featurecla))
# Remove water points from la_grid
la_grid <- la_grid[st_within(la_grid,land) %>% sapply(length) > 0,]

```

Now plot the snapshot data using the same plotting code as applied to the
`meuse` data.  This is without any map outline.

TODO:  Edit to refine the visual representation with colors, smaller bubbles,
axis labels, etc.


```{r plot fall snapshot}
#-----plot fall snapshot-----

# Follows plotting options shown above; some commented out

# log nox
ggplot(data=fall) +
  geom_sf(aes(color=ln_nox)) +
  scale_color_gradientn(colors=rainbow(4,rev=TRUE)) +
  ggtitle("ln(NOx) Concentrations (ln(ppb)") +
  theme_bw()

# now produce a bubble plot -- doesn't look great here
#bubble(fall, "ln_nox", col="blue", main = "ln(NOx) Concentrations (ln(ppb)")

# we can also produce a similar plot of the data with ggplot
ggplot(data=fall) +
  geom_sf(data=fall,aes(size=ln_nox), color='blue', alpha=1/2) +
  ggtitle("ln(NOx) Concentrations (ln(ppb)") +
  theme_bw()

# [Lianne, here is an option, not sure it is better. The ln_nox really scruntches up the scale]
  # (DLS -- leaving this code from Chris in here for now, fairly similar to above -- DLS)
  #ggplot(data=fall) + 
  #geom_sf(aes(color = exp(ln_nox)), alpha = 0.4) +
  #scale_color_continuous(type = "viridis") +
  #labs(title = "NOx Concentrations (ppb)", 
  #     color = "NOx (ppb)") + 
  #coord_equal() + 
  #theme_bw()
```

### Mapping the data

R has many map options available now.  This seems to be evolving rapidly and
there appears to be a growing number of tools available.  Some maps require an
API key which adds a layer of complexity we won't address in ENVH 556.  Stamen
maps do not require API keys, at least not yet.

Here is code to use `ggmap` with a Stamen map in the background.  The steps are
to set the region for the map and then define the borders, call the Stamen map
after choosing from a variety of options, and then overlay our data onto this.
For some basic info on using these in R, see [Getting started with Stamen maps
with ggmap](https://www.r-bloggers.com/getting-started-stamen-maps-with-ggmap/).

Note 1:  the zoom option specifies the scaling on the map.  In `ggmap` zoom can
be an integer between 1 and 21.  The smallest zooms are global and
continent-level scales, the middle ones (~10-12) are city scale, and 21 is at a
building level.

Note 2:  Maps require access to the internet to load. 

```{r LA Stamen map}
# ----LA Stamen map-----

# uses ggmap; initial version of code kindly provided by Brian High

# Obtain bounding box (left, bottom, right, top extremes of object) for la_grid
# Reproject to lambert (in meters) and extent out by 10 km
# Then convert back to lat-long, get the bounding box, convert bbox to vector
# We use this to specify how large of a basemap to download
map_bbox <- la_grid %>%
  st_transform(lambert_proj) %>%
  st_buffer(dist=10000) %>%
  st_transform(latlong_proj) %>%
  st_bbox() %>%
  as.vector()

# Download a basemap of "Stamen" tiles
map <- suppressMessages(get_stamenmap(map_bbox, zoom = 11,
                                     maptype = "toner-background"))

# Make a ggmap object from the tiles 
g <- ggmap(map, darken = c(0.5, "white")) + theme_void() 

# Plot the map
g + ggtitle("Sample Map of Los Angeles for \n the area covered by the snapshot data")

# Add snapshot locations to the map will colors for values
# Note: need to use a data.frame for this, not a spatial data object

# For some reason, geom_sf() did not work for this (bug?), so needed to use geom_point
# and manually add the extracted XY coordinates
# https://stackoverflow.com/questions/60066424/how-to-align-ggmap-crs-of-google-map-longitude-latitude

# NOx on native scale
g + geom_point(data=fall,aes(x = st_coordinates(fall)[,'X'],
                           y = st_coordinates(fall)[,'Y'],
                           col = exp(ln_nox)), alpha = 0.8) + 
    scale_color_gradientn(name = "NOx (ppb)", colors=rainbow(4,rev=TRUE)) + 
    ggtitle("Map of Los Angeles \n with the fall snapshot data")

# NOx on log scale
g + geom_point(data=fall,aes(x = st_coordinates(fall)[,'X'],
                           y = st_coordinates(fall)[,'Y'],
                           col = ln_nox), alpha = 0.8) + 
  scale_color_gradientn(name='ln(NOx)', colors=rainbow(4,rev=TRUE)) +
  ggtitle("ln(NOx) Concentrations (ln(ppb))") +
  theme_bw()


```

Note, in comparing the above map to the one displayed in Figure 2b of Mercer, a
few observations are in order:

* The gradient points in Mercer are spread out much more than in our map.  This
was done on purpose in the displays in Mercer et al to enable the viewer to see
the gradient values.  Our data have not been transformed this way.  (It would be
a good exercise to implement this...)

Now add the grid to the map.  This plot isn't particularly informative, other
than making it obvious to us that the area covered by the grid is not completely
aligned with the area covered by the map.

```{r plot the grid on the map}
# ----plot the grid on the map-----

# Note: we read the grid csv data above
# The map range is set above so no points are removed.

g + geom_point(data=la_grid,aes(x = st_coordinates(la_grid)[,'X'],
                           y = st_coordinates(la_grid)[,'Y']), alpha = 0.2) + 
  ggtitle("Map of Los Angeles \n with the grid locations overlaid") +
  theme_bw()

```


### Estimation using the snapshot data: Universal Kriging using the common model

Here we fit a UK model using the snapshot data, evaluate their quality using
cross-validation, and then produce predictions at the grid locations for later
use.

**Step 1**:  Estimate the variograms and geostatistical model parameters
(partial sill, range, nugget).  In the following chunk we fit two sets of
models:  an OK model with no trend (i.e. covariates in a LUR), and a UK model
with trend (i.e. the covariates in the common model).  We plot the variogram
fits to both models for comparison.  (Note:  this comparison is for
educationa/practice purposes only.  Scientifically we don't think an OK model of
these data is a sensible choice.)

```{r fall estimate variogram}
#-----fall estimate variogram-----

# first estimate the variogram object and assign it a name
# the default maximum distance (cutoff) is too short so set it to align w/
# values shown in Mercer
# Use lambert projection (meters) for variogram
vf <- variogram(ln_nox~1, st_transform(fall,lambert_proj), cutoff = 31000)

# Then fit a variogram model, offering to the function several different model
# options (exponential, spherical, and Matern):
# TODO:  Note that none of the variogram models appears to converge.
# Investigate.  Still true? Was it the cutoff was too short?
# [CZ: this is working for me as-is]
vf.fit <- fit.variogram(vf, vgm(c("Exp", "Sph", "Mat")))

# Find out which variogram model was selected
# Observe that it was the spherical with a range of 25,790 m, a partial sill of
# 0.1091, and a nugget of 0.0188.
vf.fit

# Plot the empirical variogram with the overlaid fit
plot(vf, vf.fit)

# now repeat for the UK model
# first estimate the variogram object and assign it a name
vfc <-
  variogram(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm,
            data = st_transform(fall,lambert_proj),
            cutoff = 31000)

# Then fit a variogram model, offering to the function several different model
# options (exponential, spherical, and Matern):
vfc.fit <- fit.variogram(vfc, vgm(c("Exp", "Sph", "Mat")))

# Find out which variogram model was selected
# Observe that it was the spherical with a range of 47,050 m, a partial sill of
# 0.0302, and a nugget of 0.01704.
vfc.fit <- fit.variogram(vfc, vgm(c("Exp", "Sph", "Mat")))

# Plot the empirical variogram with the overlaid fit
plot(vfc, vfc.fit)

# Mercer used an exponential variogram, so refit the variogram using that:
vfc.fit <- fit.variogram(vfc, vgm("Exp"))
# Observe that the exponential has a range of 79,356 m, a partial sill of
# 0.0749, and a nugget of 0.01798.  This differs from what was reported by
# Mercer.
vfc.fit 
# Plot the empirical variogram with the overlaid fit.  Observe the exponential
# and spherical fits are almost indistinguishable.
plot(vfc, vfc.fit)

```

Observe:  TODO:  ADD comments.


**Step 2**:  Cross-validate the UK models to evaluate the quality of the
predictions.  


```{r krige.cv in LA, warning=FALSE}

# clusterwise cross-validation of OK model with covariates
fall.CVcluster.ok <- krige.cv(ln_nox~1,
    st_transform(fall,lambert_proj),
    model = vf.fit,
    nfold = fall$cluster)
#stats
krige.cv.stats(fall.CVcluster.ok)
# bubble plot
krige.cv.bubble(st_transform(fall.CVcluster.ok,latlong_proj),'ln(NOx) OK: clusterwise CV residuals')


# clusterwise cross-validation of UK model with covariates
fall.CVcluster.uk <- krige.cv(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm,
    st_transform(fall,lambert_proj),
    model = vfc.fit,
    nfold = fall$cluster)
#stats
krige.cv.stats(fall.CVcluster.uk)
# bubble plot
krige.cv.bubble(st_transform(fall.CVcluster.uk,latlong_proj),'ln(NOx) UK: clusterwise CV residuals')


```


**Step 3**:  Estimate the kriging predictions.  

```{r krige in LA}
#-----krige in LA-----

kc_la <-
  krige(ln_nox ~ D2A1 + A1_50 + A23_400 + Pop_5000 + D2C + Int_3000 + D2Comm,
        st_transform(fall,lambert_proj),
        st_transform(la_grid,lambert_proj),
        model = vfc.fit)

# check out the results; predictions are in `var1.pred`
kc_la

```

### Plotting the predictions

Display the predictions on a map.  First we need to join the predictions with
the original grid in order to get the longitude and latitue coordinates.  The
second visible chunk adds our predictions to a Stamen map.  Later plots show
refinements to the plotted predictions, with smoothing and restrictions to the
area shown.



```{r plot the grid predictions on the map}
#-----plot the grid predictions on the map-----

# reproject predictons from labmert to to lat-long before plotting
kc_la <- st_transform(kc_la,latlong_proj)

# first need to merge the predictions into the la_grid in order to use.
# We can verify that the coordinates are *almost* exactly the same
all.equal(st_coordinates(kc_la), st_coordinates(la_grid))
# But not *exactly* the same.
# Likely due to floating point / rounding issues during kriging
identical(st_coordinates(kc_la), st_coordinates(la_grid))

# So when we do a strict spatial join, all the predictions are NA.
st_join(la_grid, kc_la)

# To circumvent this issue, let's join by nearest feature (point)
new_grid <- st_join(la_grid, kc_la, join=st_nearest_feature)
# Another way to join -- specifying very small tolerance -- takes longer
#new_grid <- st_join(la_grid, kc_la, join=st_equals_exact, par=1e-10)

# also create predicted NOx on the native scale
new_grid <- new_grid %>% mutate(NOx = exp(var1.pred))

# Take a look. Still has same number of features, and no NAs
new_grid
all(!is.na(new_grid$var1.pred))

# create theme
my_theme <- theme(
    legend.position = c(.98, .02),
    legend.justification = c(1, 0),
    legend.box.background = element_rect(colour = "white", size = 3),
    legend.background = element_rect(fill = "white")
    )


# make plot.  Note the relationship between NOx and highways
g + geom_point(data=new_grid,aes(x = st_coordinates(new_grid)[,'X'],
                           y = st_coordinates(new_grid)[,'Y'],
                           col = NOx), alpha = 0.2) + 
    scale_color_gradientn(name = "NOx (ppb)", colors=rainbow(4,rev=TRUE)) + 
    ggtitle("Map of Los Angeles \n with fall UK predictions overlaid as points")


```

```{r plot gridded predictions not on the water, eval=FALSE, include=FALSE}
#-----plot gridded predictions not on the water-----

# Note: The shape of the plotted prediction area does not quite match the land
# area because the LA shape used to limit the plotting to LA does not match the
# shape in the stamen tiles. This is probably because we are not using the 
# correct projection string. To get around this, we can shift the coordinates:
#    mutate(longitude = longitude - 0.02, latitude = latitude - 0.02) 
# but this is just a "kludge". What's a kludge? From Wikipedia:
# "A kludge or kluge is a workaround or quick-and-dirty solution that is clumsy, 
# inelegant, inefficient, difficult to extend and hard to maintain."
# It would be better to find the correct projection string. If you would like 
# to look into this, see this page: https://rpubs.com/alobo/getmapCRS ... 
# or make your map from a shapefile, with a known projection, as  described in 
# a different section of this document. 

# identifies locations over the water and excludes them in new_grid
#   leverages previous chunk results

# Make a SpatialPointsDataFrame from new_grid
proj_str <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"

spdf_new_grid <- SpatialPointsDataFrame(
  coords = new_grid[, c("longitude", "latitude")], 
  data = new_grid, 
  proj4string = CRS(proj_str) 
  )

# Make some geospatial objects for LA
la_county <-  map_data("county") %>% 
    filter(region == 'california', subregion == 'los angeles') %>% 
    rename("longitude"="long", "latitude"="lat") %>% 
    mutate(longitude = longitude - 0.02, latitude = latitude - 0.02)

la_county_p = Polygon(coords = la_county[, c("longitude", "latitude")])
la_county_ps = Polygons(list(la_county_p), 1)
la_county_sps = SpatialPolygons(list(la_county_ps), proj4string = CRS(proj_str))


# Subset new_grid to only include those points in LA
new_grid_in_la <- spdf_new_grid %>% 
  as_tibble() %>% 
  mutate(idx = over(spdf_new_grid, la_county_sps) ) %>% 
  drop_na(idx)
  
# Make the map
g + geom_point(aes(x = longitude, y = latitude, col = NOx),
               data = new_grid_in_la,
               alpha = 0.5) +
  ggtitle("Map of Los Angeles \n with fall UK predictions overlaid, \n omitting points over the water") +
  scale_color_gradient(name = "NOx (ppb)", low = "yellow", high = "red") +
  my_theme

```

Now we show an example plotting smooth gridded predictions on the map with polygons.
```{r plot smooth grid predictions on the map with polygons}
# ----plot smooth grid predictions on the map with polygons-----

# Interpolate to a regularly spaced grid and store as a list
# TODO:  this step is failing due to missing values and Infs not allowed so the
# rest of the chunk fails also
# [CZ: I filtered out the Inf values of la_grid, so this works now. It doesn't fix the fact that the la_grid file has -Infs]
# DLS -- Needed to back-transform the sf object to SpatialPointsDataFrame here (for interp)

new_grid_interp <- new_grid %>% as_Spatial() %>% interp(z='NOx')

# Expand grid into a data frame
new_grid_dens_expand<-
    with(new_grid_interp, expand.grid(x = x, y = y)) %>%
    mutate(z = as.vector(new_grid_interp$z),
           z = ifelse(is.na(z), 0, z))

# Note that the smoothing pattern smudges onto the water
g + stat_contour(aes(x = x, y = y, z = z, fill = ..level..), alpha = 0.07, 
                 data = new_grid_dens_expand, geom = "polygon", bins = 50) + 
    scale_fill_gradient(name = "NOx (ppb)", low = "yellow", high = "red", 
                        breaks = seq(0, 125, 25), limits = c(0, 125)) +  
    ggtitle("Map of Los Angeles \n with fall UK predictions overlaid, \n smoothed predictions (with interpolation due to missing grid points)")

```

Now show an example plotting smooth gridded predictons on the map with tiles
using stat_summary_2d:

```{r plot smooth grid predictions on the map with tiles 2, eval=FALSE, include=FALSE}
#-----plot smooth grid predictions on the map with tiles 2-----

# Note: we replace "+" with "%+%" to override the data previously mapped (i.e., replace
#   the current data frame, due to S3 method precedence issues)

## DLS - not sure what this is trying to plot, so left alone for now
g %+% new_grid + aes(x = longitude, y = latitude, z = NOx) +
    stat_summary_2d(fun = mean, alpha = 0.7, geom = "tile", bins = 30) +
    scale_fill_gradient(name = "NOx (ppb)", low = "yellow", high = "red") +
    ggtitle("Map of Los Angeles \n with fall UK predictions overlaid, \n using tiles with stat_summary_2d function") + 
    my_theme

```

Now we add code to plot the predictions on a map using shapefiles.

TODO:  This isn't currently working.  Drop?

```{r prediction map with shapefiles, eval = FALSE}
#----- prediction map with shapefiles --------


## DLS -- Link no longer works

# Define URL for shapefile data library top-level folder

data_lib <- 'http://www.dot.ca.gov/hq/tsip/gis/datalibrary/zip'

# Get a shapefile for California state highways

url <- paste(data_lib, "highway/MAY2016_State_Highway_NHS.zip", sep = "/")
shpfile_dir <- "shapefiles"
shpfile_zip <- "MAY2016_State_Highway_NHS.zip"
shpfile <- "MAY2016_State_Highway_NHS"
if (!file.exists(shpfile_zip)) {
    download(url, dest = shpfile_zip, mode="wb")
}
unzip (shpfile_zip, exdir = shpfile_dir)

ca_hwy_shapefile <- rgdal::readOGR(shpfile_dir, shpfile)
ca_hwy_shapefile_df <- fortify(ca_hwy_shapefile)

# Get map projection string

ca_hwy_proj <- proj4string(ca_hwy_shapefile)

# Get a shapefile for California urban areas

url <- paste(data_lib, "Boundaries/2010_adjusted_urban_area.zip", sep = "/")
shpfile_dir <- "shapefiles"
shpfile_zip <- "2010_adjusted_urban_area.zip"
shpfile <- "2010_adjusted_urban_area"
if (!file.exists(shpfile_zip)) {
    download(url, dest = shpfile_zip, mode="wb")
}
unzip (shpfile_zip, exdir = shpfile_dir)

ca_urb_areas_shapefile <- rgdal::readOGR(file.path(shpfile_dir, shpfile), shpfile)

# Get map projection string

ca_urb_proj <- proj4string(ca_urb_areas_shapefile)

# Convert LA urban area polygon to a SpatialPolygon

ca_urb_sp <- ca_urb_areas_shapefile[
    grepl('^Los Angeles', ca_urb_areas_shapefile$NAME10), ]@polygons
ca_urb_sp <- SpatialPolygons(Srl = ca_urb_sp, proj4string = CRS(ca_urb_proj))

# Transform projection of urban areas SpatialPolygons object if necessary

if (ca_hwy_proj != ca_urb_proj) {
    ca_urb_sp <- spTransform(ca_urb_sp, ca_hwy_proj)
}

# Convert LA urban area SpatialPolygons object to a data frame for plotting

ca_urb_df <- fortify(ca_urb_sp)

# Convert highway shapefile data frame to a SpatialPointsDataFrame

ca_hwy_spdf <- ca_hwy_shapefile_df
coordinates(ca_hwy_spdf) <- ~ long + lat
proj4string(ca_hwy_spdf) <- CRS(ca_hwy_proj)

# Subset highways data frame to only include those points in LA for plotting

ca_hwy_spdf_subset <- ca_hwy_spdf[!is.na(over(ca_hwy_spdf, ca_urb_sp)), ]
ca_hwy_df <- as.data.frame(ca_hwy_spdf_subset)

# Convert expanded test grid data frame to a SpatialPointsDataFrame

new_grid_dens_expand_spdf <- new_grid_dens_expand
coordinates(new_grid_dens_expand_spdf) <- ~ x + y
proj4string(new_grid_dens_expand_spdf) <- CRS(ca_hwy_proj)

# Subset expanded test grid data frame to only include points in LA for plotting

new_grid_dens_expand_spdf_subset <-
    new_grid_dens_expand_spdf[!is.na(over(new_grid_dens_expand_spdf, ca_urb_sp)),]
new_grid_dens_expand_df <-
    as.data.frame(new_grid_dens_expand_spdf_subset)
    
# Plot the LA border (ca_urb_df), LA highways (ca_hwy_df), and predictions

ca_map <- ggplot() +
    geom_path(data = ca_urb_df, 
              aes(x = long, y = lat, group = group),
              color = 'black', size = 0.5) + 
    geom_path(data = ca_hwy_df, 
              aes(x = long, y = lat, group = group),
              color = 'blue', size = 0.4) + 
    stat_contour(aes(x = x, y = y, z = z, fill = ..level..), alpha = 0.05, 
                 data = new_grid_dens_expand_df, geom = "polygon", bins = 50) + 
    scale_fill_gradient(name = "NOx (ppb)", low = "yellow", high = "red", 
                        breaks = seq(0, 125, 25), limits = c(0, 125)) +  
    ggtitle("Map of Los Angeles \n with fall UK predictions overlaid, \n smoothed predictions over a shapefile") + 
    theme_void() + my_theme
map_projected <- ca_map + coord_map()
print(map_projected)

```

#### Ordinary kriging using the residuals from a LUR

The idea is to use a traditional linear model to fit the common model, save the
residuals from this model, and then import this model into `gstat` and fit an OK
model.

TODO:  ADD this.  This is a homework problem.

#### Variograms to compare with Mercer

The idea is to try to produce the variograms like Mercer.  Need to use the
breaks function to allow for very fine scale distances to capture the gradient
structure.

TODO:  ADD this.  This is an optional homework problem.

# Practice Session

During class we will review the output above.  Please come prepared to ask questions.

# Homework Exercises 

TODO:  Review and update

1.  Fit a LUR model (using the common model covariates) to the season-specific
snapshot data.  (One model per season.)  Take the residuals from those models
and evaluate them:
    a.  Estimate an empirical binned variogram to the residuals using default
    bins.  Plot this and discuss.
    b.  **Optional**:  Try a different set of bins for the variogram, making
    sure you create a few bins at the shorter distances (within the range of 0
    to approximately 650 meters).  Plot this and discuss.
    c.  Discuss what you have learned from plotting the variograms.  Is there
    evidence of spatial structure in the data?  Do you get different insights
    from each variogram?
    
2.  **Optional extra credit**:  Using 10-fold cross-validation with the cluster
variable to define the CV groups, estimate predictions from a 2-step model using
the common model covariates.  For this model you need to separately create
cross-validated predictions from the LUR model and from the OK model of the LUR
residuals (of the full season-specfic dataset), and sum these to compare with
the observed ln_nox data.

3.	Write a basic summary of your understanding of how universal kriging differs
from land use regression.  What additional insights do you get from the Mercer
et al paper now that you have done some analyses using the snapshot data?

4.	Discuss your thoughts on the most useful summaries to show from an exposure
prediction analysis.


# Appendix

## Session information

```{r session.info}
#-----session information-----

# print R session information
sessionInfo()

```

## Embedded code

```{r code.appendix, ref.label = knitr::all_labels(), echo = TRUE, eval = FALSE, , include=TRUE}
#-----code appendix-----
```

## Functions defined 

```{r functions, eval = TRUE}
#-----functions-----

# Show the names of all functions defined in the .Rmd
# (e.g. loaded in the environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)

```


