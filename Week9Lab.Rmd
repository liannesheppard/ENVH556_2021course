---
title: "Week 9 Lab:  Time Series Data and Analysis Strategies"
author: "Elena Austin and Chris Zuidema for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%B %d, %Y')`"
output: 
    html_document:
        df_print: "paged"
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
    rm(res)
}

```

```{r load.libraries, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, dplyr, tidyr, readr, stringr, ggplot2, hms, 
               lubridate, purrr, tsibble, feasts, slider, ggmap, ggrepel)

```

```{r directory.organization.read.data, echo=FALSE, warning=FALSE}
#-----directory organization and read data-----

# specify working directory
project_path <- getwd()

# specify data directory
data_dir <- file.path(project_path, "Datasets", "mobile_monitoring")

# Download files if not already present
if (!dir.exists(data_dir)) {
    
  # create "Datasets" and "mobile_monitoring" directories if they do not already
  # exist
  dir.create(file.path(project_path, "Datasets", "mobile_monitoring"), 
             showWarnings = FALSE, recursive = TRUE)

  # specify URL of zip file
  url <- "https://faculty.washington.edu/sheppard/envh556/Datasets/mobile_monitoring/Jul19MOV-UP.zip"
  
  # name destination file
  dest <- file.path(data_dir, "Jul10MOV-UP.zip")
  
  # download zip
  download.file(url = url, destfile = dest)
  
  # unzip file
  unzip(zipfile = dest, exdir = data_dir)

  # delete zip files (optional)
  unlink(dest)
  unlink(str_subset("__MACOSX", list.files(data_dir)), recursive = TRUE)
  
  }
```

<!-- TODO:  THINGS TO ADD/MODIFY/REFRAME IN LAB:

# Time alignment before combining dataset
(all instruments are 1-sec interval currently...)

# Spatial Smoothing
average by distance covered?

# Removing the Influence of Local Exhaust Plumes
rolling 25th percentile method used by Choi et al. (2012)
outlier resistant statistics such as the median

# Characterizing Background
Another task in mobile monitoring studies is to characterize "background"
concentrations.

# Lab replicates concepts in:
[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/)
[full text](https://amt.copernicus.org/articles/7/2169/2014/amt-7-2169-2014.pdf)

-->

# Introduction and Purpose

The purpose of this lab is to practice working with time series data, develop
facility in importing and merging data of varying time scales, smoothing
approaches, and some analysis tools to reduce the complexity of these data. In
addition, as with all the labs in this course, you should focus on writing up a
coherent lab report that addresses the scientific context and scientific
questions.

In this lab exercise we will use data collected as part of the MOV-UP mobile
monitoring campaign. This data was collected on 1-second and 10-second time
scales. The lab will walk you through the steps of importing the raw instrument
data. The MOV-UP study has collected data from the following instruments: 

  * GPS tracker
  * particle counter (P-Trak, model 8525, TSI Inc., MN)
  * condensation particle counter (CPC, model 3007, TSI Inc., MN)
  * black carbon aethelometer (microAeth AE51) 
  * particle sizing spectrometer (NanoScan)
  * carbon dioxide ($CO_2$) (SenseAir K-30-FS)

The full suite of instruments and their characteristics of the platform used in
this study can be found in [Riley et al.,
2014](https://doi.org/10.1016/j.atmosenv.2014.09.018), [Larson et al.,
2017](https://doi.org/10.1016/j.atmosenv.2016.12.037), and [Xiang et
al.,2020](https://doi.org/10.1021/acs.est.0c00612). The remainder of the code in
this week's lab uses the GPS, P-Trak, and $CO_2$ measurements.


# Dates and Times

Managing dates and times in exposure assessment is a common challenge because
there are a variety of common date and time formats used to log instrument data.
You've no doubt encountered the following representations of dates:

   * `r format(Sys.time(), '%d %b, %Y')`
   * `r format(Sys.time(), '%m/%d/%Y')`
   * `r format(Sys.time(), '%Y-%m-%d')`

With direct-reading instruments made all over the world, you'll have the
potential to encounter many devices that use different date and time formats.
You'll find it advantageous to convert everything to a standardized format,
including the time zone. Consistently using a standardized date and time format
will make it less likely that you'll interpret a date and time incorrectly.

Date objects in `R` are of class `Date`, and compound date and time ("datetime")
objects, are of class `POSIXct` (shown as `dttm` when a tibble is printed to the
console). Let's take a look at  a `POSIXct` object and show its class: 

```{r POSIXct.objects}
#-----POSIXct objects-----

# show POSIXct object
Sys.time()

# show class
class(Sys.time())

```

Internally, date and datetime objects store the number of seconds (for
`POSIXct`) or days (for `Date`) since January 1st 1970 at midnight: 
`r as.numeric(Sys.time())` seconds and `r as.numeric(Sys.Date())` days. When
printed to the console, they are displayed in a human-readable format along the
the International Organization for Standardization (ISO) 8601 date and time
standard. ISO 8601 provides an unambiguous, well-known, and well-defined method
of representing dates and times.

It is often necessary to define date/time components or format of a datetime
object specifically for each instrument. In addition, the time scale of each
instrument can be different, and it is necessary to ensure a common schedule
prior to merging the data. You'll also likely encounter dates/times with various
time zones, daylight savings conventions, and perhaps even issues with
simultaneous measurements on either side of the international date line.
Accounting for these issues will make it easier to handle date and datetime
variables.


# Identify Files of Interest, Read Files & Wrangle

Here, we'll focus on data from Car 1 (there were 2 cars), for your assignment,
consider exploring data from a different car.

```{r gather.file.names.for.import}
#-----gather file names for import-----

# get filename paths for data files
filenames <- list.files(data_dir) %>% str_subset("car1")

```

## GPS

```{r gps, message=FALSE}
#-----gps-----

# Get GPS file name  
gps_file <- str_subset(filenames, "GPS")

# read GPS data
gps <- read_csv(file.path(data_dir, gps_file)) %>%
   
   # set column names
   set_names(c("date", "time", "latitude", "longitude", "altitude", "speed")) %>% 
   
   # create datetime column (notice we must specify the timezone)
   mutate(datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )

# show dataframe
gps

```

## P-Trak

These instruments collect ultrafine particles (UFPs) at the 1 second time
resolution. An unscreened P-Trak collects particles in the range of 20 - 1,000
nm while the screened P-Track collects particles in the range of 50-1,000 nm.
Thus, having both allows some rudimentary understanding of particle size
distributions. The instruments measure particle number concentration, with units
expressed as $particles/cm^3$, $pt/cm^3$, or $1/cm^3$

```{r p.trak, message=FALSE}
#-----p-trak-----

# get p-trak not-screened file names
ptrak_noscreen_file <- filenames %>% 
   str_subset("PT") %>% 
   str_subset("scrnd|Scrnd|Screened|screen", negate = TRUE)

# read data
ptrak_noscreen <-  read_tsv(file.path(data_dir, ptrak_noscreen_file), skip = 29) %>% 
   
   # name columns
   set_names(c("date","time", "ptrak_noscreen_conc")) %>% 
   
   # modify date column and create datetime column (notice we must specify the
   # date format)
   mutate(date = as.Date(date, format = "%m/%d/%Y"), 
          datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )


# get p-trak-screened file names and read in data
ptrak_screen_file <- filenames %>% 
   str_subset("scrnd|Scrnd|Screened|screen")

# read data
ptrak_screen <-  read_tsv(file.path(data_dir, ptrak_screen_file), skip = 29) %>% 
   
   # name columns
   set_names(c("date","time", "ptrak_screen_conc")) %>% 
   
   # modify date column and create datetime column
   mutate(date = as.Date(date, format = "%m/%d/%Y"), 
          datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )


# show dataframes
ptrak_noscreen
ptrak_screen

```

## Carbon Dioxide

```{r co2, message=FALSE}
#-----co2-----

# get CO2 file name
co2_file <- str_subset(filenames, "CO2")

# read data, specify time as character because of time parsing issue
co2 <- read_tsv(file.path(data_dir, co2_file), skip = 1, 
                col_types = cols(`System_Time_(h:m:s)` = col_character()) ) %>% 
   
   # name columns
   set_names(c("date", "time", "co2_conc", "h2o_conc", "temp", "pressure", 
              "co2_absorp", "flow", "X9") ) %>% 
   
   # select columns of interest
   select(date, time, co2_conc) %>% 
   
   # address time parsing (some minute values have only one digit) and 
   # create datetime column
   mutate(time = as_hms(time), 
          datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )

# show dataframe
co2

```


# Merge & Inspect Instrument Data

```{r merge.datasets}
#-----merge datasets-----

# create a list of all data to merge 
instrument_list <- list(gps = gps, ptrak_noscreen = ptrak_noscreen, 
                        ptrak_screen = ptrak_screen, co2 = co2)

# use `purrr::reduce` to run `full_join()` multiple times on `instrument_list`
all_data <- reduce(instrument_list, full_join) %>% 
   
   # reorder columns so datetime is first
   relocate(datetime) %>% 
   
   # arrange data in datetime order
   arrange(datetime)

# show data
all_data

```

```{r data.availability.plot}
#-----data.availability.plot-----

# Lianne, I looked into this because Dave suggested it, but it doesn't seem so
# informative

# create temporary dataframe for ggplot
temp <- all_data %>% 
  pivot_longer(cols = c(contains("conc"), latitude), 
               names_to = "measurement")

# make plot
ggplot(data = temp, 
       aes(x = datetime, y = measurement, color = factor(measurement)) ) +
  geom_point() +
  theme_bw()

```



## Missing Data

How much missing data do we have? What are the likely sources of data
missingness in our data?

```{r missing}
#-----missing-----

lapply(all_data, function(i){ 
   
   tibble( 
          # sum missing
          n_miss = sum(is.na(i)), 
          
          # percent missing
          perc_miss = round(n_miss/length(i) * 100, 1)
          )
   }) %>% 
   
   # bind list
   bind_rows(.id = "variable")

```

## Plot Data 

Let's make a quick visualization to get an idea of the data we're working with.
What features do you see?

```{r plot.data}
#-----plot data-----

# transform data from wide to long for ggplot
ggplot(data = all_data %>% 
         pivot_longer(cols = contains("_conc"), 
                      names_to = "pollutant", 
                      values_to = "concentration"), 
       aes(x = datetime, y = concentration) ) + 
  
  # plot each pollutant in a facet
  facet_wrap(~pollutant, ncol = 1, scales = "free_y") + 
  
  # specify type of plot
  geom_line() +
  
  # specify theme
  theme_bw()

```


# Time Interval

Our data appeared to be on a similar 1-second time interval. Let's confirm:

```{r time.stamps}
#-----time stamps

# we'll use the instrument data list
lapply(instrument_list, function(i){
   
   # get vector of datetimes
   datetimes <- i[["datetime"]]
   
   # get a vector of the differences between consecutive measurements
   time_diff <- datetimes[2:length(datetimes)] - datetimes[1:length(datetimes)-1]
   
   # summarize differences
   table(time_diff)
   
}) %>% 
   
   # use list names
   set_names(names(instrument_list))

```

It seems that some of our missingness is due to instrument reporting. Some
measurements did not occur at 1-second intervals. From this we can also see
there are duplicate measurements for some instruments (difference of zero
seconds).

Lastly, it appears that the GPS instrument was not turned on at the same time as
the other instruments (there is no GPS data at the beginning of the dataframe).
This may be due to the fact that the instruments were turned on for "warm-up"
before the measurement campaign started.

Let's drop data missing the GPS variables for convenience, but is this the best
thing to under all circumstances and for all variables?

```{r drop.missing.gps}
#-----drop missing gps-----

all_data <- drop_na(all_data, latitude, longitude)

```


# Timeseries data

Timeseries data are marked by measurements that are indexed to a time component.
There are many `R` standards for timeseries data: `ts`, `xts`, `data.frame`,
`data.table`, `tibble`, `zoo`, `tsibble`, `tibbletime` or `timeSeries`. The
package `tsbox` has many useful functions for converting between these
timeseries formats.

We've focused most of our effort on `tidyverse` tools this term, so let's
concentrate on the `tsibble`, `feasts`, and `slider` package functions.

```{r try.tsibble}
#-----try tsibble----

# use "try" here so document knits
try( as_tsibble(all_data, index = datetime) )

# inspect duplicate rows
duplicates(all_data, index = datetime)

```

`tsibble` alerts us of our time issues and prompts us to deal with them, so
let's remove the duplicates (you could average duplicates also). 

```{r to.tsibble}
#-----to tsibble-----

# remove duplicate rows, and convert to `tsibble`
ts_data <- all_data %>% 
  distinct() %>% 
  as_tsibble(index = datetime)

```

Let's also fill the gaps in time. In other words, the index column `datetime`
needs to be unique and complete. Adding these new time rows introduces
"explicit" NAs to the data, so you'll then have a choice about if and how to
fill the non-time data in. `tidyr::fill()` can help with this task, and the
choice may be more or less important depending on the timescale you're working
on, the amount of missingness, and if there are consecutive measurements
missing. Given the small amount of missing and the one-second timescale of the
data, it's not unreasonable to introduce a few rows with NAs here.

```{r fill.ts}
#-----fill ts-----

# count rows before filling
paste("number of rows before filling gaps:", nrow(ts_data))

ts_data <- fill_gaps(ts_data) 
  
  # # if you wanted to "fill" the new explicitly missing data here is an option
  # # (add a `%>%` to the function above). Note this could have downstream
  # # effects.
  # fill(co2_conc, .direction = "down")

# count rows after filling
paste("number of rows after filling gaps:", nrow(ts_data))

# show dataframe
ts_data

```

# Temporal Autocorrelation

Timeseries data is often correlated in time - measurements taken close in time
to one another are more similar than measurements taken farther apart. An
autocorrelation plot helps identify at what lag (or time interval) data are less
correlated. In the following plot, the blue lines indicate bounds on the
autocorrelation of these data. At what time do the $CO_2$ appear less
correlated?

```{r autocorrelation}
#-----autocorrelation-----

ts_data %>% ACF(co2_conc, lag_max = 60) %>% autoplot()

```


# Temporal Smoothing with New Time Scales 

A common task with timeseries data is averaging to different timescales. From
the autocorrelation plot above, we can see at a lag of 60 seconds, the $CO_2$
concentrations are less correlated. So let's convert our 1-second data to some
longer timescales. One package that helps with averaging tasks is `slider`.

For the first average we'll use the timescale period (minutes) to calculate
1-minute averages. Though with other data, this could also be months, weeks,
years, etc. This is a "block average" based on an attribute of the data. For
example, if you wanted monthly averages, this method would be "aware" of the
fact there are a different number of days in each month. For the second average
(2-minutes) we'll demonstrate a moving ("sliding" or "rolling") average, where a
number of neighbors before and after a value of interest are used to compute an
average.

Notice how both functions preserve the input dataframe size. For the period
average we can then easily merge the new column with our existing dataframe
(however, you'll notice values are repeated 60 times for each minute). The
moving average, works nicely with `mutate()`.

## `slider`

```{r slider}
#-----slider-----

ts_data <- ts_data %>% 
  
  # left join new 1-minute period values
  left_join(

    # use `slider` to calculate 1-minute concentrations by period
    slide_period_dfr(ts_data, ts_data$datetime, "minute",
                     ~tibble(datetime = floor_date(.x$datetime),
                             co2_conc_one = mean(.x$co2_conc)
                             ) ),
    by = "datetime" ) %>% 
  
  # add a datetime column with 1-minute precision to help if you choose to
  # `group_by()` and `summarise()` 1-minute averages
  mutate(datetime_one = ymd_hm(format(datetime, "%Y-%m-%d %H:%M"), 
                               tz = "US/Pacific")) %>%
  
  # use `slider` to calculate 2 minute moving averages (this works because we've
  # made sure our datetime index is unique and complete)
  mutate(co2_conc_two = slide_dbl(co2_conc, ~mean(.x, na.rm = TRUE), 
                             .before = 60, .after = 60) ) 

# # slide_period also works like this, but returns a vector of different length
# # than the original inputs
# with(ts_data, 
#      slide_period_dbl(.x = co2_conc, .i = datetime, 
#              .period = "minute", .f = mean, na.rm = TRUE) 
#      )

# show dataframe
ts_data

```

## `tsibble`

Next, let's take a look at a `tsibble` approach for 5-minute averages. Notice
that the size of the dataframe decreases.

```{r new.timescale}
#-----new timescales-----

ts_new <- ts_data %>% 
  
  # get the "floor" of each datetime row (unfortunately `tsibble` doesn't let us
  # use "datetime" for this new variable name)
  index_by(datetime_new = floor_date(datetime, unit = "5mins")) %>%
  
  # summarise the mean of rows across all dataframe columns
  summarise(across(where(is.numeric), mean, na.rm = TRUE ), .groups = "drop") %>% 
  
  # rename to get "datetime" variable name back
  rename(datetime = datetime_new)
  
# show dataframe
ts_new

# check 5-minute average autocorrelation
#ts_new %>% ACF(co2_conc) %>% autoplot()
```

All our example data are on the 1-second timescale; however, it is common to
have multiple timescales in a suite of direct-reading instruments. Using the
techniques above, you'll be able to average different time intervals to a common
schedule. Once the measurements are on a shared timescale, they can be
merged/joined as we have shown in the "Merge & Inspect Instrument Data" section.

## Plot Different Timescales

Let's inspect the timeseries of our different averages:

```{r plot.different.time.averages}
#-----plot different time averages-----

# we're going to cheat a little here and just manually specify each series rather
# than make a dataframe with all the plotting data combined

ggplot() + 
  
  # plot original 1-second data
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc, color = "blue")) + 
  
  # plot `slider` 1-min period averages
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc_one, color = "black")) +
  
  # plot `slider` 2-min moving averages
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc_two, color = "red")) +
  
  # plot `tsibble` 5-minute block averages
  geom_line(data = ts_new, aes(x = datetime, y = co2_conc, color = "darkgreen")) +  
  # specify legend values manually
  scale_color_identity(name = "Time Average", 
                       breaks = c("blue", "black", "red", "darkgreen"), 
                       labels = c("1-sec", "1-min", "2-min", "5-min"), 
                       guide = "legend") +
  
  # labels
  labs(x = "Time", 
       y = "CO2 (ppm)" 
       ) +
  
  # theme
  theme_bw() + 
  theme(legend.position = "bottom")

```


# Map concentrations

Our data was from a mobile monitoring campaign - so let's map the
concentrations.

Here is a helper function for the `bbox` argument of `get_stamenmap()`. It takes
a dataframe, finds the latitude and longitude columns, makes calculations to add
a little "extra" space to the plotting area, and outputs an object used to
define the extent of the mapping region. You might find it helpful in the
future. It requires `stringr`and `tidyr`.

```{r define.make_bbox.function}
#-----define make_bbox function------

make_bbox <- function(.data, width_extra = 0.0001, height_extra = 0.00005){
  
  # dependencies
  pacman::p_load(stringr, tidyr)
  
  # find lat and long column names
  long <- str_subset(string = names(.data), pattern = "Long|long")
  lat <- str_subset(string = names(.data), pattern = "Lat|lat")
  
  # drop rows with NA
  temp <- .data %>% drop_na(all_of(c(long, lat)))
  
  # create a named vector with the corners of the plotting region
  out <- c(left = min(temp[[long]]) + width_extra*min(temp[[long]]),
           bottom = min(temp[[lat]]) - height_extra*min(temp[[lat]]),
           right = max(temp[[long]]) - width_extra*min(temp[[long]]),
           top = max(temp[[lat]]) + height_extra*min(temp[[lat]]) )
}

```

In the following chunk we define the bounding box (extent of the mapping area),
download the stamen map tiles (if they have not already been downloaded), and
save it for later. If the map file has already been downloaded and saved, we can
just load the `map.rds` file. This will save you some time and from
unnecessarily repeating file downloads as you work to develop your code. Note if
you've already downloaded and saved a `map.rds` file, then want to modify the
underlying map somehow, you'll need to delete the `map.rds` file first.

```{r get.map.objects}
#-----get map objects-----

# define the bounding box for the map using `make_bbox()` 
bbox <- make_bbox(ts_data)

# download stamen map if needed, otherwise just load it from a previous download
if (!file.exists(file.path(data_dir, "map.rds"))) {
  
  # make a map of the base layer of stamen tiles (without showing messages)
  map <- suppressMessages(get_stamenmap(bbox, zoom = 14, maptype = "terrain"))
  
  # save map to the mobile_monitoring data directory so you don't need to
  # download it again
  saveRDS(map, file = file.path(data_dir, "map.rds") )
  
  }

# read map file
map <- readRDS(file.path(data_dir, "map.rds"))

# make the map image from the stamen map tiles
basemap <- ggmap(map, darken = c(0.5, "white")) + theme_void()

```

To help represent the time-varying nature of the data in this lab, let's collect
the times of some of our measurements to help our visualization.

```{r get time stamps}
#-----get time stamps-----

map_times <- ts_data %>% 
  
  # convert to `tibble` because we're breaking up our `tsibble`
  as_tibble() %>%
  
  # filter to rows with :00, :15, :30, and :45 minute times
  filter(str_detect(time, pattern = ":00:00|:15:00|:30:00|:45:00")) %>% 
  
  # make a map_time variable, dropping the seconds 
  # (`$` indicates the character pattern should be at the end of the string)
  mutate(map_time = str_remove(time, pattern = ":00$"))

```

## Map Carbon Dioxide Concentrations

```{r map.concentration, warning=FALSE}
#-----map concentrations----

# map concentrations
map_co2 <- basemap +
    
# locations with concentrations
geom_point(data = ts_data %>% drop_na(co2_conc), 
           aes(x = longitude, y = latitude, color = co2_conc))+ 

# add measurement times to map
geom_text_repel(data = map_times,
                aes(x = longitude, y = latitude, label = map_time), 
                force_pull = -.02) +

# color scale
scale_color_continuous(low = "#56B1F7", high = "#132B43") +  

# labels
labs(color = "CO2 (ppm)") +

# theme
theme_void() 

# show map
map_co2

```


## Map P-Trak Ratios
<!-- Lianne & Elena, 
THIS IS NOT IN BRANTLEY, BUT IS IT SOMETHING TO KEEP? IT LIKELY NEEDS
MODIFICATION AS I WAS JUST FIGURING OUT/DRAFTING SOME CODE -->

As the $CO_2$ map shows, the data we've collected represent concentrations over
4 hours, meaning some of the variability we're observing could be a mix of
spatial (i.e. the variability we see in the map) and temporal (i.e. the
variability observed in timeseries plots) processes.

Next, we'll use the P-Trak data to explore the idea of pollutant ratios...

```{r map.ratios}
#-----map ratios----

# calculate ratio
ts_data <- ts_data %>% 
  mutate(ptrak_ratio = ptrak_screen_conc / ptrak_noscreen_conc)

# map concentrations
map_ratios <- basemap +
    
# locations with concentrations
geom_point(data = ts_data %>% drop_na(ptrak_ratio), 
           aes(x = longitude, y = latitude, color = ptrak_ratio)) + 

# color scale
scale_color_continuous(low = "#56B1F7", high = "#132B43") +  

# labels
labs(color = "P-trak Ratio \nscreened:unscreened") +

# theme
theme_void() 

# show map
map_ratios

```

# Calculating Background & Identifying Peak Concentrations

As we've seen, mobile monitoring data has a mix of temporal and spatial
variability. In an effort to disentangle temporal confounding from spatial
analysis, we can calculate the background concentration of a pollutant, then
subtract it from the observed concentrations. Peak identification can then be
performed on the background adjusted data to identify single or multi-pollutant
extreme features (for example, departures above a particular quantile of single
pollutant features vs departure above background of a multivariate
representation such as a PCA feature).

## Calculating Background as a Percentile of the Data 

[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/)
summarize a number of methods to calculate background concentrations, including
those of [Bukowiecki et al.,](https://doi.org/10.1016/S1352-2310(02)00694-5),
which includes taking the 5^th^ percentile of 1 or 5 min averages.

```{r calculate background.as.moving.percentile}
#-----calculate background as moving percentile-----

# specify the time in minutes from which to calculate the percentile
time_min <- 5

# specify the percentile considered to be background
perc <- 0.05

# we can use use `slider` to calculate the percentile equal to background
ts_data <- ts_data %>% 
  mutate(background = slide_dbl(co2_conc, 
                                 ~quantile(.x, probs = perc, na.rm = TRUE), 
                                 .before = (time_min*60)/2, 
                                 .after = (time_min*60)/2), 
         co2_adj = co2_conc - background) 

```

Here, we've taken `r time_min` minutes as the time interval and the `r 
perc*100`^th^ percentile of the data to calculate the background $CO_2$
concentration.

### Plot Carbon Dioxide without Background

```{r plot.co2.minus.background}
#-----plot co2 minus background-----

ggplot(ts_data) + 
  
  # plot the percentile-adjusted data
  geom_line(aes(x = datetime, y = co2_adj)) +
  
  # plot the loess-adjusted data
  #geom_line(aes(x = datetime, y = co2_adj_loess), color = "red") + 
  
  # add reference line
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +
  
  # labels
  labs(x = "Time", y = "CO2 Minus Background (ppm)") +
  
  # theme
  theme_bw()

```


## Identifying Peak Concentrations

<!-- Lianne & Elena, 
I'm still working on this section. I read through the
papers mentioned below to implement the minimum and percentile peak detection
methods, but it still isn't clear to me how to implement -->

With background removed from the $CO_2$ concentrations, we can now use the
adjusted data to identify concentration peaks. Concentration peaks indicate
potential sources of the pollutant. In this case, a $CO_2$ peak could be a large
vehicle like a truck.

[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/) also
summarize methods to identify peak concentrations. Here, we'll demonstrate a
rolling minimum ([Kolb et al., 2004](https://doi.org/10.1021/es030718p)) and the
rolling 25^th^ percentile of the data ([Choi et al.,
2012](https://doi.org/10.1016/j.atmosenv.2012.07.084)).

```{r id.peaks}
#-----id peaks-----

# specify the time in minutes from which to calculate the percentile
time_min <- 5

# specify the percentile considered to be background
perc <- 0.25

# we can use use `slider` to calculate these rolling statistics
ts_data <- ts_data %>% 
  mutate(co2_peak_min = slide_dbl(co2_adj, 
                                 ~min(.x, na.rm = TRUE), 
                                 .before = (time_min*60)/2, 
                                 .after = (time_min*60)/2), 
         co2_peak_perc = slide_dbl(co2_adj, 
                                   ~quantile(.x, probs = perc, na.rm = TRUE), 
                                   .before = (time_min*60)/2, 
                                   .after = (time_min*60)/2), 
         ) 

```









## Loess Smoother {-}
<!-- Lianne & Elena, 
ALSO NOT IN BRANTLEY, BUT SUGGESTED BY LIANNE. IT SEEMS LIKE I'M DOING
SOMETHING WRONG OR MISSING SOMETHING... MOVED HERE FOR NOW, BUT DROP?-->

Another way to remove background in mobile monitoring data is to fit a smoother to the data and subtract the fitted values from the observations. 

```{r loess.smoother, eval=FALSE, include=FALSE}
#-----loess smoother-----

# specify loess model
mdl <- loess(co2_conc ~ seq_along(time), data = ts_data, span = 0.1)

# add predictions and adjusted concentrations to ts_data
ts_data <- ts_data %>% 
  bind_cols(loess = predict(mdl, ts_data)) %>% 
  mutate(co2_adj_loess = co2_conc-loess)

```



# Practice

<!-- Lianne & Elena, Please make suggestions below -->

1. Choose a new pollutant. Investigate the autocorrelation: 
    a. plot the 1-second autocorrelation plot
    b. choose a new timescale
    c. check the autocorrelation with the new timescale


# Code Appendix

## Session Information

```{r session.info}
#-----session info: beginning of Code Appendix-----

sessionInfo()

```

## Code in the R Markdown file

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}
#-----appendix code-----

```

## User-written functions loaded in the R Markdown environment

```{r functions.used.in.this.Rmd, eval = TRUE}
#-----functions used in this Rmd-----

# Show the names of all functions used (loaded in the current environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)

```
