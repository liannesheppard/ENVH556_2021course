---
title: "Week 9 Lab:  Time Series Data and Analysis Strategies"
author: "Elena Austin and Chris Zuidema for ENVH 556"
date: "Winter 2021; Updated `r format(Sys.time(), '%B %d, %Y')`"
output: 
    html_document:
        df_print: "paged"
        fig_caption: yes
        toc: true
        toc_depth: 3
        number_sections: true
---

```{r setup, include=FALSE}
#-----setup-----

# set knitr options
knitr::opts_chunk$set(echo = TRUE)

# clear work space of all objects and unload all extra (non-base) packages
rm(list = ls(all = TRUE))
if (!is.null(sessionInfo()$otherPkgs)) {
    res <- suppressWarnings(
        lapply(paste('package:', names(sessionInfo()$otherPkgs), sep=""),
               detach, character.only=TRUE, unload=TRUE, force=TRUE))
    rm(res)
}

```

```{r load.libraries, echo=FALSE, include=FALSE, eval=TRUE}
#-----load libraries-----

# Load pacman into memory, installing as needed
my_repo <- 'http://cran.r-project.org'
if (!require("pacman")) {install.packages("pacman", repos = my_repo)}

# Load the other packages, installing as needed.
pacman::p_load(knitr, dplyr, tidyr, readr, stringr, ggplot2, 
               lubridate, purrr, tsibble, feasts, slider, ggmap, ggrepel)

```

```{r directory.organization.read.data, echo=FALSE, warning=FALSE}
#-----directory organization and read data-----

# specify working directory
project_path <- getwd()

# create "Datasets" and "mobile_monitoring" directories if the do not already exist  
# TODO:  do we need both lines or just the second one?
dir.create(file.path(project_path,"Datasets"), showWarnings=FALSE, recursive = TRUE)
dir.create(file.path(project_path, "Datasets", "mobile_monitoring"), showWarnings=FALSE, recursive = TRUE)

# specify data directory
data_dir <- file.path(project_path, "Datasets", "mobile_monitoring")

# TODO:  add code to download multiple files at once.  Here is the code for a single file named "snapshot.file":
# # Specify file name and path:
# snapshot.file <- "snapshot_3_5_19.csv"
# snapshot.path <- file.path(data_path, snapshot.file)
# # Download the file if it is not already present
# if (!file.exists(snapshot.path)) {
#     url <- paste("https://faculty.washington.edu/sheppard/envh556/Datasets", 
#                  snapshot.file, sep = '/')
#     download.file(url = url, destfile = snapshot.path)
# }

```

<!-- TODO:  THINGS TO ADD/MODIFY/REFRAME IN LAB:

# Time alignment before combining dataset
(all instruments are 1-sec interval currently...)

# Spatial Smoothing
average by distance covered?

# Removing the Influence of Local Exhaust Plumes
rolling 25th percentile method used by Choi et al. (2012)
outlier resistant statistics such as the median

# Characterizing Background
Another task in mobile monitoring studies is to characterize "background"
concentrations.

# Lab replicates concepts in:
[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/)
[full text](https://amt.copernicus.org/articles/7/2169/2014/amt-7-2169-2014.pdf)

-->

# Introduction and Purpose

The purpose of this lab is to practice working with time series data, develop
facility in importing and merging data of varying time scales, smoothing
approaches, and some analysis tools to reduce the complexity of these data. In
addition, as with all the labs in this course, you should focus on writing up a
coherent lab report that addresses the scientific context and scientific
questions.

In this lab exercise we will use data collected as part of the MOV-UP mobile
monitoring campaign. This data was collected on 1-second and 10-second time
scales. The lab will walk you through the steps of importing the raw instrument
data. The MOV-UP study has collected data from the following instruments: 

  * GPS tracker
  * particle counter (P-Trak, model 8525, TSI Inc., MN)
  * condensation particle counter (CPC, model 3007, TSI Inc., MN)
  * black carbon aethelometer (microAeth AE51) 
  * particle sizing spectrometer (NanoScan)
  * carbon dioxide ($CO_2$) (SenseAir K-30-FS)

The full suite of instruments and their characteristics of the platform used in
this study can be found in [Riley et al.,
2014](https://doi.org/10.1016/j.atmosenv.2014.09.018), [Larson et al.,
2017](https://doi.org/10.1016/j.atmosenv.2016.12.037), and [Xiang et 
al.,2020](https://doi.org/10.1021/acs.est.0c00612). The remainder of the code in this week's lab uses the GPS, P-Trak, and $CO_2$ measurements.


# Dates and Times

Managing dates and times in exposure assessment is a common challenge because
there are a variety of common date and time formats used to log instrument data.
You've no doubt encountered the following representations of dates:

   * `r format(Sys.time(), '%d %b, %Y')`
   * `r format(Sys.time(), '%m/%d/%Y')`
   * `r format(Sys.time(), '%Y-%m-%d')`

With direct-reading instruments made all over the world, you'll have the
potential to encounter many devices that use different date and time formats.
You'll find it advantageous to convert everything to a standardized format,
including the time zone. Consistently using a standardized date and time format
will make it less likely that you'll interpret a date and time incorrectly.

We suggest the International Organization for Standardization (ISO) date and
time standard, which provides an unambiguous, well-known, and well-defined
method of representing dates and times. Dates are often not stored in ISO 8601
date format, making it necessary to define date/time components specifically for
each instrument. In addition, the time scale of each instrument can be
different, and it is necessary to ensure a common time base prior to merging the
data.

(DLS comment: Mention time zones, international date line, and daylight savings?)

# Identify Files of Interest, Read Files & Wrangle

Here, we'll focus on data from Car 1 (there were 2 cars), for your assignment,
consider exploring data from a different car.

```{r gather.file.names.for.import}
#-----gather file names for import-----

# get filename paths for data files
filenames <- list.files(data_dir) %>% str_subset("car1")

```

## GPS

```{r gps, message=FALSE}
#-----gps-----

# Get GPS file name  
gps_file <- str_subset(filenames, "GPS")

# read GPS data
gps <- read_csv(file.path(data_dir, gps_file)) %>%
   
   # set column names
   set_names(c("date", "time", "latitude", "longitude", "altitude", "speed")) %>% 
   
   # create datetime column
   mutate(datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )

# show dataframe
gps

```

## P-Trak

<!-- Elena & Lianne, can one of you add a sentence or two here about the
importance/reasoning behind the screen and not-screened P-Trak -->

```{r p.trak, message=FALSE}
#-----p-trak-----

# get p-trak not-screened file names
ptrak_noscreen_file <- filenames %>% 
   str_subset("PT") %>% 
   str_subset("scrnd|Scrnd|Screened|screen", negate = TRUE)

# read data
## Best practice is not to hard-code header lengths
## The `skip=<string>` argument in data.table::fread is very handy for this
## See: https://github.com/kaufman-lab/trap_data_mgmt/blob/master/data_import_lib.R
## For examples of how Elena, Brian, and Dave have dealt with instrument files
ptrak_noscreen <-  read_tsv(file.path(data_dir, ptrak_noscreen_file), skip = 29) %>% 
   
   # name columns
   set_names(c("date","time", "ptrak_noscreen_conc")) %>% 
   
   # modify date column and create datetime column
   mutate(date = as.Date(date, format = "%m/%d/%Y"), 
          datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )


# get p-trak-screened file names and read in data
ptrak_screen_file <- filenames %>% 
   str_subset("scrnd|Scrnd|Screened|screen")

# read data
ptrak_screen <-  read_tsv(file.path(data_dir, ptrak_screen_file), skip = 29) %>% 
   
   # name columns
   set_names(c("date","time", "ptrak_screen_conc")) %>% 
   
   # modify date column and create datetime column
   mutate(date = as.Date(date, format = "%m/%d/%Y"), 
          datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )


# show dataframes
ptrak_noscreen
ptrak_screen

```

## Carbon Dioxide

```{r co2, message=FALSE}
#-----co2-----

# get CO2 file name
co2_file <- str_subset(filenames, "CO2")

# read data, specify time as character because of time parsing issue
co2 <- read_tsv(file.path(data_dir, co2_file), skip = 1, 
                col_types = cols(`System_Time_(h:m:s)` = col_character()) ) %>% 
   
   # name columns
   set_names(c("date", "time", "co2_conc", "h2o_conc", "temp", "pressure", 
              "co2_absorp",  "flow", "X9") ) %>% 
   
   # select columns of interest
   select(date, time, co2_conc) %>% 
   
   # address time parsing (some minute values have only one digit)
  ## Lubridate should be able to handle this part more or less automatically
  ## See examples in trap_data_mgmt repo
   separate(time, into = c("h", "m", "s"), sep = ":") %>% 
   mutate(m = if_else(str_count(m) == 1, paste0("0", m), m)) %>% 
   unite(c(h, m, s), col = "time", sep = ":") %>% 
   mutate(time = parse_time(time)) %>%
   
   # create datetime column
   mutate(datetime = as.POSIXct(paste(date, time), tz = "US/Pacific") )

# show dataframe
co2

```


# Merge & Inspect Instrument Data

```{r merge.datasets}
#-----merge datasets-----

# create a list of all data to merge 
instrument_list <- list(gps = gps, ptrak_noscreen = ptrak_noscreen, 
                        ptrak_screen = ptrak_screen, co2 = co2)

# use `purrr::reduce` to run `full_join()` multiple times on `instrument_list`
all_data <- reduce(instrument_list, full_join) %>% 
   
   # reorder columns so datetime is first
   relocate(datetime) %>% 
   
   # arrange data in datetime order
   arrange(datetime)

# show data
all_data

```


## Missing Data

How much missing data do we have? What are the likely sources of data
missingness in our data?

```{r missing}
#-----missing-----

lapply(all_data, function(i){ 
   
   tibble( 
          # sum missing
          n_miss = sum(is.na(i)), 
          
          # percent missing
          perc_miss = round(n_miss/length(i) * 100, 1)
          )
   }) %>% 
   
   # bind list
   bind_rows(.id = "variable")

## It might be nice somewhere in here to have a data availability vs. time plot

```

## Plot Data 

Let's make a quick visualization to get an idea of the data we're working with.
What features do you see?

```{r plot.data}
#-----plot data-----

# transform data from wide to long for ggplot
ggplot(data = all_data %>% 
         pivot_longer(cols = contains("_conc"), 
                      names_to = "pollutant", 
                      values_to = "concentration"), 
       aes(x = datetime, y = concentration) ) + 
  
  # plot each pollutant in a facet
  facet_wrap(~pollutant, ncol = 1, scales = "free_y") + 
  
  # specify type of plot
  geom_line() +
  
  # specify theme
  theme_bw()

```


# Time Interval

Our data appeared to be on a similar 1-second time interval. Let's confirm:

```{r time.stamps}
#-----time stamps

# we'll use the instrument data list
lapply(instrument_list, function(i){
   
   # get vector of datetimes
   datetimes <- i[["datetime"]]
   
   # get a vector of the differences between consecutive measurements
   time_diff <- datetimes[2:length(datetimes)] - datetimes[1:length(datetimes)-1]
   
   # summarize differences
   table(time_diff)
   
}) %>% 
   
   # use list names
   set_names(names(instrument_list))

```

It seems that some of our missingness is due to instrument reporting. Some
measurements did not occur at 1-second intervals. From this we can also see
there are duplicate measurements for some instruments (difference of zero
seconds).

Lastly, it appears that the GPS instrument was not turned on at the same time as
the other instruments (there is no GPS data at the beginning of the dataframe).
This may be due to the fact that the instruments were turned on for "warm-up"
before the measurement campaign started.

Let's drop data missing the GPS variables for convenience, but is this the best
thing to under all circumstances and for all variables?

```{r drop.missing.gps}
#-----drop missing gps-----

all_data <- drop_na(all_data, latitude, longitude)

```


# Timeseries data

Timeseries data are marked by measurements that are indexed to a time component.
There are many `R` standards for timeseries data: `ts`, `xts`, `data.frame`,
`data.table`, `tibble`, `zoo`, `tsibble`, `tibbletime` or `timeSeries`. The
package `tsbox` has many useful functions for converting between these
timeseries formats.

We've focused most of our effort on `tidyverse` tools this term, so let's
concentrate on the `tsibble`, `feasts`, and `slider` package functions.

```{r try.tsibble}
#-----try tsibble----

# use "try" here so document knits
try( as_tsibble(all_data, index = datetime) )

# inspect duplicate rows
duplicates(all_data, index = datetime)

```

`tsibble` alerts us of our time issues and prompts us to deal with them, so
let's remove the duplicates (you could average duplicates also). 

```{r to.tsibble}
#-----to tsibble-----

# remove duplicate rows, and convert to `tsibble`
ts_data <- all_data %>% 
  distinct() %>% 
  as_tsibble(index = datetime)

```

Let's also fill the gaps in time. In other words, the index column `datetime`
needs to be unique and complete. Adding these new time rows introduces
"explicit" NAs to the data, so you'll then have a choice about if and how to
fill the non-time data in. `tidyr::fill()` can help with this task, and the
choice may be more or less important depending on the timescale you're working
on, the amount of missingness, and if there are consecutive measurements
missing. Given the small amount of missing and the one second timescale, it's
not unreasonable to introduce a few rows with NAs here.

```{r fill.ts}
#-----fill ts-----


## Could be nice to take a row count before and after to see how many were filled in

ts_data <- fill_gaps(ts_data) 
  
  # # if you wanted to "fill" the new explicitly missing data here is an option
  # # (add a `%>%` to the function above). Note this could have downstream
  # # effects.
  # fill(co2_conc, .direction = "down")

# show dataframe
ts_data

```

# Temporal Autocorrelation

Timeseries data is often correlated in time - measurements taken close in time
to one another are more similar than measurements taken farther apart. An
autocorrelation plot helps identify at what lag (or time interval) data are less
correlated. In the following plot, the blue lines indicate bounds on the
autocorrelation of these data. At what time do the $CO_2$ appear less
correlated?

```{r autocorrelation}
#-----autocorrelation-----

ts_data %>% ACF(co2_conc, lag_max = 60) %>% autoplot()

```


# Temporal Smoothing with New Time Scales 

A common task with timeseries data is averaging to different timescales. From
the autocorrelation plot above, we can see at a lag of 60 seconds, the $CO_2$
concentrations are less correlated. So let's convert our 1-second data to some
longer timescales. One package that helps with averaging tasks is `slider`.

For the first average we'll use the timescale period (minutes) to calculate
1-minute averages. Though with other data, this could also be months, weeks,
years, etc. This is a "block average" based on an attribute of the data. For
example, if you wanted monthly averages, this method would be "aware" of the
fact there are a different number of days in each month. For the second average
(2-minutes) we'll demonstrate a moving ("sliding" or "rolling") average, where a
number of neighbors before and after a value of interest are used to compute an
average.

Notice how both functions preserve the input dataframe size. For the period
average we can then easily merge the new column with our existing dataframe
(however, you'll notice values are repeated 60 times for each minute). The
moving average, works nicely with `mutate()`.

## `slider`

```{r slider}
#-----slider-----

## For 1-minute averaging, it might be nice to either have a time-to-the-minute column
## So that you can easily pare down your data later
## Or to do a mutate/summarize() to the minute into a new table

ts_data <- ts_data %>% 
  
  # left join new 1-minute period values
  left_join(

    # use `slider` to calculate 1-minute concentrations by period
    slide_period_dfr(ts_data, ts_data$datetime, "minute",
                     ~tibble(datetime = floor_date(.x$datetime),
                             co2_conc_one = mean(.x$co2_conc)
                             ) ),
    by = "datetime" ) %>%
  
  # use `slider` to calculate 2 minute moving averages (this works becuase we've
  # made sure our datetime index is unique and complete)
  mutate(co2_conc_two = slide_dbl(co2_conc, ~mean(.x, na.rm = TRUE), 
                             .before = 60, .after = 60) ) 

# # slide_period also works like this, but returns a vector of different length
# # than the original inputs
# with(ts_data, 
#      slide_period_dbl(.x = co2_conc, .i = datetime, 
#              .period = "minute", .f = mean, na.rm = TRUE) 
#      )

# show dataframe
ts_data

```

## `tsibble`

Next, let's take a look at a `tsibble` approach for 5-minute averages. Notice
that the size of the dataframe decreases.

```{r new.timescale}
#-----new timescales-----

## This is really handy!

ts_new <- ts_data %>% 
  
  # get the "floor" of each datetime row (unfortunately `tsibble` doesn't let us
  # use "datetime" for this new variable name)
  index_by(datetime_new = floor_date(datetime, unit = "5mins")) %>%
  
  # summarise the mean of rows across all dataframe columns
  summarise(across(where(is.numeric), mean, na.rm = TRUE ), .groups = "drop") %>% 
  
  # rename to get "datetime" variable name back
  rename(datetime = datetime_new)
  
# show dataframe
ts_new

# check 5-minute average autocorrelation
#ts_new %>% ACF(co2_conc) %>% autoplot()
```

All our example data are on the 1-second timescale; however, it is common to
have multiple timescales in a suite of direct-reading instruments. Using the
techniques above, you'll be able to average different time intervals to a common
schedule. Once the measurements are on a shared timescale, they can be
merged/joined as we have shown in the "Merge & Inspect Instrument Data" section.

## Plot Different Timescales

Let's inspect the timeseries of our different averages:

```{r plot.different.time.averages}
#-----plot different time averages-----

# we're going to cheat a little here and just manually specify each series rather
# than make a dataframe with all the plotting data combined

ggplot() + 
  
  # plot original 1-second data
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc, color = "blue")) + 
  
  # plot `slider` 1-min period averages
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc_one, color = "black")) +
  
  # plot `slider` 2-min moving averages
  geom_line(data = ts_data, aes(x = datetime, y = co2_conc_two, color = "red")) +
  
  # plot `tsibble` 5-minute block averages
  geom_line(data = ts_new, aes(x = datetime, y = co2_conc, color = "darkgreen")) +  
  # specify legend values manually
  scale_color_identity(name = "Time Average", 
                       breaks = c("blue", "black", "red", "darkgreen"), 
                       labels = c("1-sec", "1-min", "2-min", "5-min"), 
                       guide = "legend") +
  
  # labels
  labs(x = "Time", 
       y = "CO2 (ppm)" 
       ) +
  
  # theme
  theme_bw() + 
  theme(legend.position = "bottom")

```


# Map concentrations

Our data was from a mobile monitoring campaign - so let's map the
concentrations.

Here is a helper function for the `bbox` argument of `get_stamenmap()`. It takes
a dataframe, finds the latitude and longitude columns, makes calculations to add
a little "extra" space to the plotting area, and outputs an object used to
define the extent of the mapping region. You might find it helpful in the
future. It requires `stringr`and `tidyr`.

```{r define.make_bbox.function}
#-----define make_bbox function------

make_bbox <- function(.data, width_extra = 0.0001, height_extra = 0.00005){
  
  # dependencies
  pacman::p_load(stringr, tidyr)
  
  # find lat and long column names
  long <- str_subset(string = names(.data), pattern = "Long|long")
  lat <- str_subset(string = names(.data), pattern = "Lat|lat")
  
  # drop rows with NA
  temp <- .data %>% drop_na(all_of(c(long, lat)))
  
  # create a named vector with the corners of the plotting region
  out <- c(left = min(temp[[long]]) + width_extra*min(temp[[long]]),
           bottom = min(temp[[lat]]) - height_extra*min(temp[[lat]]),
           right = max(temp[[long]]) - width_extra*min(temp[[long]]),
           top = max(temp[[lat]]) + height_extra*min(temp[[lat]]) )
}

```

(I've split up getting the map objects from mapping them so you can run the
mapping chunk interactively without re-downloading the stamen map.)

```{r get.map.objects}
#-----get map objects-----

# define the bounding box for the map using make_bbox 
bbox <- make_bbox(ts_data)

## This takes a little while to download. Could do something like
## if (!file.exists('map.rds')) {saveRDS('map.rds')} ... readRDS(''map.rds'')
# in this area of the code to save time when re-running the script
## Or, could decrease the zoom level a bit
# make a map of the base layer of stamen tiles (without downloading messages)
map <- get_stamenmap(bbox, zoom = 14, maptype = "terrain") %>% suppressMessages()

# make the map image from the stamen map tiles
basemap <- ggmap(map, darken = c(0.5, "white")) + theme_void()

```

To help represent the time-varying nature of the data in this lab, let's collect
the times of some of our measurements to help our visualization.

```{r get time stamps}
#-----get time stamps-----

map_times <- ts_data %>% 
  
  # convert to `tibble` because we're breaking up our `tsibble`
  as_tibble() %>%
  
  # filter to rows with :00, :15, :30, and :45 minute times
  filter(str_detect(time, pattern = ":00:00|:15:00|:30:00|:45:00")) %>% 
  
  # make a map_time variable, dropping the seconds 
  # (`$` indicates the character pattern should be at the end of the string)
  mutate(map_time = str_remove(time, pattern = ":00$"))

```

## Map Carbon Dioxide Concentrations

```{r map.concentration, warning=FALSE}
#-----map concentrations----

# map concentrations
map_co2 <- basemap +
    
# locations with concentrations
geom_point(data = ts_data %>% drop_na(co2_conc), 
           aes(x = longitude, y = latitude, color = co2_conc))+ 

# add measurement times to map
geom_text_repel(data = map_times,
                aes(x = longitude, y = latitude, label = map_time), 
                force_pull = -.02) +

# color scale
scale_color_continuous(low = "#56B1F7", high = "#132B43") +  

# labels
labs(color = "CO2 (ppm)") +

# theme
theme_void() 

# show map
map_co2

```


## Map P-Trak Ratios
<!-- Lianne & Elena, 
THIS IS NOT IN BRANTLEY, BUT IS IT SOMETHING TO KEEP? IT LIKELY NEEDS
MODIFICATION AS I WAS JUST FIGURING OUT/DRAFTING SOME CODE -->

As the $CO_2$ map shows, the data we've collected represent concentrations over 4 hours, meaning some of the variability we're observing could be a mix of spatial (i.e. the variability we see in the map) and temporal (i.e. the variability observed in timeseries plots) processes.

Next, we'll use the P-Trak data to explore the idea of pollutant ratios...

```{r map.ratios}
#-----map ratios----

# calculate ratio
ts_data <- ts_data %>% 
  mutate(ptrak_ratio = ptrak_screen_conc / ptrak_noscreen_conc)

# map concentrations
map_ratios <- basemap +
    
# locations with concentrations
geom_point(data = ts_data %>% drop_na(ptrak_ratio), 
           aes(x = longitude, y = latitude, color = ptrak_ratio)) + 

# color scale
scale_color_continuous(low = "#56B1F7", high = "#132B43") +  

# labels
labs(color = "P-trak Ratio \nscreened:unscreened") +

# theme
theme_void() 

# show map
map_ratios

```

# Calculating Background & Identifying Peak Concentrations

As we've seen, mobile monitoring data has a mix of temporal and spatial
variability. In an effort to disentangle temporal confounding from spatial
analysis, we can calculate the background concentration of a pollutant, then
subtract it from the observed concentrations. Peak identification can then be
performed on the background adjusted data to identify single or multi-pollutant
extreme features (for example, departures above a particular quantile of single
pollutant features vs departure above background of a multivariate
representation such as a PCA feature).

## Calculating Background as a Percentile of the Data 

[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/)
summarize a number of methods to calculate background concentrations, including
those of [Bukowiecki et al.,](https://doi.org/10.1016/S1352-2310(02)00694-5),
which includes taking the 5^th^ percentile of 1 or 5 min averages.

```{r calculate background.as.moving.percentile}
#-----calculate background as moving percentile-----

# specify the time in minutes from which to calculate the percentile
time_min <- 5

# specify the percentile considered to be background
perc <- 0.05

# we can use use `slider` to calculate the percentile equal to background
ts_data <- ts_data %>% 
  mutate(background = slide_dbl(co2_conc, 
                                 ~quantile(.x, probs = perc, na.rm = TRUE), 
                                 .before = (time_min*60)/2, 
                                 .after = (time_min*60)/2), 
         co2_adj = co2_conc - background) 

```

Here, we've taken `r time_min` minutes as the time interval and the `r 
perc*100`^th^ percentile of the data to calculate the background $CO_2$
concentration.

### Plot Carbon Dioxide without Background

```{r plot.co2.minus.background}
#-----plot co2 minus background-----

ggplot(ts_data) + 
  
  # plot the percentile-adjusted data
  geom_line(aes(x = datetime, y = co2_adj)) +
  
  # plot the loess-adjusted data
  #geom_line(aes(x = datetime, y = co2_adj_loess), color = "red") + 
  
  # add reference line
  geom_hline(yintercept = 0, color = "gray", linetype = "dashed") +
  
  # labels
  labs(x = "Time", y = "CO2 Minus Background (ppm)") +
  
  # theme
  theme_bw()

```


## Identifying Peak Concentrations

<!-- Lianne & Elena, 
I'm still working on this section. I read through the
papers mentioned below to implement the minimum and percentile peak detection
methods, but it still isn't clear to me how to implement -->

With background removed from the $CO_2$ concentrations, we can now use the
adjusted data to identify concentration peaks. Concentration peaks indicate
potential sources of the pollutant. In this case, a $CO_2$ peak could be a large
vehicle like a truck.

[Brantley et al., 2014](https://amt.copernicus.org/articles/7/2169/2014/) also
summarize methods to identify peak concentrations. Here, we'll demonstrate a
rolling minimum ([Kolb et al., 2004](https://doi.org/10.1021/es030718p)) and the
rolling 25^th^ percentile of the data ([Choi et al.,
2012](https://doi.org/10.1016/j.atmosenv.2012.07.084)).

```{r id.peaks}
#-----id peaks-----

# specify the time in minutes from which to calculate the percentile
time_min <- 5

# specify the percentile considered to be background
perc <- 0.25

# we can use use `slider` to calculate these rolling statistics
ts_data <- ts_data %>% 
  mutate(co2_peak_min = slide_dbl(co2_adj, 
                                 ~min(.x, na.rm = TRUE), 
                                 .before = (time_min*60)/2, 
                                 .after = (time_min*60)/2), 
         co2_peak_perc = slide_dbl(co2_adj, 
                                   ~quantile(.x, probs = perc, na.rm = TRUE), 
                                   .before = (time_min*60)/2, 
                                   .after = (time_min*60)/2), 
         ) 

```









## Loess Smoother {-}
<!-- Lianne & Elena, 
ALSO NOT IN BRANTLEY, BUT SUGGESTED BY LIANNE. IT SEEMS LIKE I'M DOING
SOMETHING WRONG OR MISSING SOMETHING... MOVED HERE FOR NOW, BUT DROP?-->

Another way to remove background in mobile monitoring data is to fit a smoother to the data and subtract the fitted values from the observations. 

```{r loess.smoother, eval=FALSE, include=FALSE}
#-----loess smoother-----

# specify loess model
mdl <- loess(co2_conc ~ seq_along(time), data = ts_data, span = 0.1)

# add predictions and adjusted concentrations to ts_data
ts_data <- ts_data %>% 
  bind_cols(loess = predict(mdl, ts_data)) %>% 
  mutate(co2_adj_loess = co2_conc-loess)

```



# Practice

<!-- Lianne & Elena, Please make suggestions below -->

1. Choose a new pollutant. Investigate the autocorrelation: 
    a. plot the 1-second autocorrelation plot
    b. choose a new timescale
    c. check the autocorrelation with the new timescale


# Code Appendix

## Session Information

```{r session.info}
#-----session info: beginning of Code Appendix-----

sessionInfo()

```

## Code in the R Markdown file

```{r appendix.code, ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE, tidy=TRUE, tidy.opts=list(width.cutoff=60), include=T}
#-----appendix code-----

```

## User-written functions loaded in the R Markdown environment

```{r functions.used.in.this.Rmd, eval = TRUE}
#-----functions used in this Rmd-----

# Show the names of all functions used (loaded in the current environment)
lsf.str()

# Show the definitions of all functions loaded into the current environment  
lapply(c(lsf.str()), getAnywhere)

```
